{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prax860/QKD_Optimisation_GNN_fork/blob/main/QGNN_Trial_02%20-%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-6rMLy1lnBCQ",
        "outputId": "6e474b74-01d4-4c33-e4cc-3b0dc044b065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-2.1.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting qiskit-aer\n",
            "  Downloading qiskit_aer-0.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.16.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.3.8)\n",
            "Collecting stevedore>=3.0.0 (from qiskit)\n",
            "  Downloading stevedore-5.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit) (4.15.0)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit-aer) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Downloading qiskit-2.1.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_aer-0.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stevedore, rustworkx, qiskit, torch_geometric, qiskit-aer\n",
            "Successfully installed qiskit-2.1.2 qiskit-aer-0.17.1 rustworkx-0.17.1 stevedore-5.5.0 torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install qiskit qiskit-aer torch torch_geometric networkx matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjvkAHvgnBnX",
        "outputId": "423a2276-69d7-4587-a1a2-722c3081f451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating QKD network...\n",
            "Creating and training model...\n",
            "Using device: cpu\n",
            "\n",
            "Fold 1/5\n",
            "Epoch 10: Train Loss = 1.2765, Val Loss = 1.3223, AUC = 0.6714, AP = 0.6598\n",
            "Epoch 20: Train Loss = 1.0874, Val Loss = 1.2170, AUC = 0.7314, AP = 0.7100\n",
            "Epoch 30: Train Loss = 0.9114, Val Loss = 1.1794, AUC = 0.7551, AP = 0.7328\n",
            "Epoch 40: Train Loss = 0.7958, Val Loss = 1.1877, AUC = 0.7636, AP = 0.7067\n",
            "Epoch 50: Train Loss = 0.7485, Val Loss = 1.1216, AUC = 0.7958, AP = 0.7832\n",
            "Epoch 60: Train Loss = 0.7147, Val Loss = 1.1491, AUC = 0.7847, AP = 0.7377\n",
            "Epoch 70: Train Loss = 0.7344, Val Loss = 1.1948, AUC = 0.7613, AP = 0.7073\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 2/5\n",
            "Epoch 10: Train Loss = 0.7703, Val Loss = 1.0891, AUC = 0.8100, AP = 0.7775\n",
            "Epoch 20: Train Loss = 0.6559, Val Loss = 1.1822, AUC = 0.7891, AP = 0.7406\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 3/5\n",
            "Epoch 10: Train Loss = 0.6826, Val Loss = 1.0303, AUC = 0.8344, AP = 0.7981\n",
            "Epoch 20: Train Loss = 0.6292, Val Loss = 1.1659, AUC = 0.8011, AP = 0.7609\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 4/5\n",
            "Epoch 10: Train Loss = 0.6629, Val Loss = 1.0796, AUC = 0.7971, AP = 0.7039\n",
            "Epoch 20: Train Loss = 0.6274, Val Loss = 0.9300, AUC = 0.8628, AP = 0.8322\n",
            "Epoch 30: Train Loss = 0.6027, Val Loss = 0.9499, AUC = 0.8478, AP = 0.8134\n",
            "Epoch 40: Train Loss = 0.6459, Val Loss = 1.1120, AUC = 0.7984, AP = 0.7470\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 5/5\n",
            "Epoch 10: Train Loss = 0.6809, Val Loss = 1.0397, AUC = 0.8300, AP = 0.7956\n",
            "Epoch 20: Train Loss = 0.6826, Val Loss = 0.8696, AUC = 0.8631, AP = 0.7900\n",
            "Early stopping triggered\n",
            "Generating visualizations and analysis...\n",
            "\n",
            "Analysis complete. Results saved in: qkd_results\n",
            "\n",
            "Summary Statistics:\n",
            "Number of nodes: 50\n",
            "Number of edges: 718\n",
            "Average degree: 28.72\n",
            "Average key rate: 1.04e+03 bits/s\n",
            "Average QBER: 0.061\n",
            "Model AUC: 0.811 ± 0.036\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATv2Conv, TransformerConv\n",
        "from torch_geometric.utils import to_undirected, negative_sampling\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
        "from scipy.constants import h, c\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def bce_with_logits_loss(pos_out, neg_out):\n",
        "    \"\"\"Custom BCE loss for link prediction\"\"\"\n",
        "    pos_loss = F.binary_cross_entropy_with_logits(\n",
        "        pos_out, torch.ones_like(pos_out)\n",
        "    )\n",
        "    neg_loss = F.binary_cross_entropy_with_logits(\n",
        "        neg_out, torch.zeros_like(neg_out)\n",
        "    )\n",
        "    return pos_loss + neg_loss\n",
        "\n",
        "def to_networkx(data):\n",
        "    \"\"\"Convert PyG data to NetworkX graph\"\"\"\n",
        "    G = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        G.add_edge(edge_index[0, i], edge_index[1, i])\n",
        "    return G\n",
        "\n",
        "class AdvancedQuantumChannelSimulator:\n",
        "    def __init__(self, distance, wavelength=1550e-9, fiber_loss=0.2,\n",
        "                 detector_efficiency=0.1, dark_count_rate=1e-6,\n",
        "                 atmospheric_visibility=None):\n",
        "        self.distance = distance\n",
        "        self.wavelength = wavelength\n",
        "        self.fiber_loss = fiber_loss\n",
        "        self.detector_efficiency = detector_efficiency\n",
        "        self.dark_count_rate = dark_count_rate\n",
        "        self.atmospheric_visibility = atmospheric_visibility\n",
        "        self.photon_energy = h * c / wavelength\n",
        "\n",
        "    def calculate_channel_loss(self):\n",
        "        fiber_loss_db = self.fiber_loss * self.distance\n",
        "        fiber_transmission = 10 ** (-fiber_loss_db/10)\n",
        "\n",
        "        if self.atmospheric_visibility:\n",
        "            beam_divergence = 1.22 * self.wavelength / 0.1\n",
        "            geometric_loss = (0.1 / (beam_divergence * self.distance)) ** 2\n",
        "            atmospheric_loss = np.exp(-3.91 * self.distance / self.atmospheric_visibility)\n",
        "            total_transmission = fiber_transmission * geometric_loss * atmospheric_loss\n",
        "        else:\n",
        "            total_transmission = fiber_transmission\n",
        "\n",
        "        return total_transmission\n",
        "\n",
        "    def simulate_bb84_protocol(self, num_pulses=10000, mean_photon_number=0.1):\n",
        "        channel_transmission = self.calculate_channel_loss()\n",
        "        received_photons = np.random.poisson(\n",
        "            mean_photon_number * channel_transmission * self.detector_efficiency,\n",
        "            num_pulses\n",
        "        )\n",
        "        dark_counts = np.random.poisson(self.dark_count_rate, num_pulses)\n",
        "        total_counts = received_photons + dark_counts\n",
        "        basis_matches = np.random.choice([0, 1], num_pulses, p=[0.5, 0.5])\n",
        "        qber = 0.5 * (1 - np.exp(-2 * self.distance / 100))\n",
        "        errors = np.random.choice([0, 1], num_pulses, p=[1-qber, qber])\n",
        "        matched_pulses = total_counts * basis_matches\n",
        "        raw_key_rate = np.sum(matched_pulses) / num_pulses\n",
        "        final_key_rate = raw_key_rate * (1 - 2 * h2(qber))\n",
        "\n",
        "        return {\n",
        "            'qber': qber,\n",
        "            'raw_key_rate': raw_key_rate,\n",
        "            'final_key_rate': final_key_rate,\n",
        "            'channel_loss_db': -10 * np.log10(channel_transmission),\n",
        "            'dark_count_probability': np.mean(dark_counts > 0)\n",
        "        }\n",
        "\n",
        "def h2(x):\n",
        "    \"\"\"Binary entropy function\"\"\"\n",
        "    return -x * np.log2(x) - (1-x) * np.log2(1-x) if 0 < x < 1 else 0\n",
        "\n",
        "class AdvancedQKDNetwork:\n",
        "    def __init__(self, num_nodes=50):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.positions = self._generate_realistic_topology()\n",
        "\n",
        "    def _generate_realistic_topology(self):\n",
        "        centers = np.random.multivariate_normal(\n",
        "            mean=[0, 0],\n",
        "            cov=[[100, 0], [0, 100]],\n",
        "            size=3\n",
        "        )\n",
        "\n",
        "        positions = []\n",
        "        for _ in range(self.num_nodes):\n",
        "            center = centers[np.random.randint(0, 3)]\n",
        "            pos = center + np.random.multivariate_normal(\n",
        "                mean=[0, 0],\n",
        "                cov=[[10, 0], [0, 10]]\n",
        "            )\n",
        "            positions.append(pos)\n",
        "\n",
        "        return np.array(positions)\n",
        "\n",
        "    def generate_graph_data(self):\n",
        "        distances = np.zeros((self.num_nodes, self.num_nodes))\n",
        "        for i in range(self.num_nodes):\n",
        "            for j in range(i + 1, self.num_nodes):\n",
        "                distances[i, j] = distances[j, i] = np.linalg.norm(\n",
        "                    self.positions[i] - self.positions[j]\n",
        "                )\n",
        "\n",
        "        edges = []\n",
        "        edge_attrs = []\n",
        "\n",
        "        for i in range(self.num_nodes):\n",
        "            for j in range(i + 1, self.num_nodes):\n",
        "                if distances[i, j] < 100:\n",
        "                    simulator = AdvancedQuantumChannelSimulator(\n",
        "                        distance=distances[i, j],\n",
        "                        atmospheric_visibility=20000 if np.random.random() < 0.2 else None\n",
        "                    )\n",
        "                    results = simulator.simulate_bb84_protocol()\n",
        "\n",
        "                    if results['final_key_rate'] > 0:\n",
        "                        edges.append([i, j])\n",
        "                        edge_attrs.append([\n",
        "                            results['final_key_rate'],\n",
        "                            results['qber'],\n",
        "                            distances[i, j],\n",
        "                            results['channel_loss_db'],\n",
        "                            results['dark_count_probability']\n",
        "                        ])\n",
        "\n",
        "        edge_index = torch.tensor(edges).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
        "\n",
        "        G = nx.Graph()\n",
        "        G.add_edges_from(edges)\n",
        "\n",
        "        node_features = []\n",
        "        for i in range(self.num_nodes):\n",
        "            features = [\n",
        "                self.positions[i, 0],\n",
        "                self.positions[i, 1],\n",
        "                G.degree(i) if i in G else 0,\n",
        "                nx.betweenness_centrality(G).get(i, 0) if i in G else 0\n",
        "            ]\n",
        "            node_features.append(features)\n",
        "\n",
        "        return Data(\n",
        "            x=torch.tensor(node_features, dtype=torch.float),\n",
        "            edge_index=to_undirected(edge_index),\n",
        "            edge_attr=edge_attr,\n",
        "            pos=torch.tensor(self.positions, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "class AdvancedQKDLinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, edge_attr_channels, hidden_channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = TransformerConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(edge_attr_channels, hidden_channels),\n",
        "            torch.nn.LayerNorm(hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "        )\n",
        "\n",
        "        self.link_predictor = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * hidden_channels, hidden_channels),\n",
        "            torch.nn.LayerNorm(hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(hidden_channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        # Process edge features for all edges\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "\n",
        "        return x, edge_features\n",
        "\n",
        "    def decode(self, z, edge_features, edge_label_index):\n",
        "        src, dst = edge_label_index\n",
        "\n",
        "        # Handle negative sampling case\n",
        "        if edge_features.size(0) != edge_label_index.size(1):\n",
        "            # For negative samples, create dummy edge features\n",
        "            edge_features = edge_features.mean(dim=0, keepdim=True).repeat(edge_label_index.size(1), 1)\n",
        "\n",
        "        node_features = torch.cat([\n",
        "            z[src],\n",
        "            z[dst],\n",
        "            edge_features\n",
        "        ], dim=-1)\n",
        "        return self.link_predictor(node_features).squeeze(-1)\n",
        "\n",
        "def train_and_evaluate(model, data, num_epochs=200, k_folds=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    all_results = []\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    edge_attr = data.edge_attr.cpu().numpy()\n",
        "    unique_edges = set()\n",
        "    edge_to_idx = {}\n",
        "\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        edge = tuple(sorted([edge_index[0, i], edge_index[1, i]]))\n",
        "        if edge not in unique_edges:\n",
        "            unique_edges.add(edge)\n",
        "            edge_to_idx[edge] = len(edge_to_idx)\n",
        "\n",
        "    unique_edges = list(unique_edges)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(unique_edges)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "\n",
        "        train_edges = [unique_edges[i] for i in train_idx]\n",
        "        val_edges = [unique_edges[i] for i in val_idx]\n",
        "\n",
        "        # Convert to numpy arrays first\n",
        "        train_edge_index = np.array([[edge[0], edge[1]] for edge in train_edges]).T\n",
        "        train_edge_attr = np.array([edge_attr[edge_to_idx[edge]] for edge in train_edges])\n",
        "\n",
        "        val_edge_index = np.array([[edge[0], edge[1]] for edge in val_edges]).T\n",
        "        val_edge_attr = np.array([edge_attr[edge_to_idx[edge]] for edge in val_edges])\n",
        "\n",
        "        # Convert to tensors\n",
        "        train_edge_index = torch.from_numpy(train_edge_index).to(device)\n",
        "        train_edge_attr = torch.from_numpy(train_edge_attr).float().to(device)\n",
        "        val_edge_index = torch.from_numpy(val_edge_index).to(device)\n",
        "        val_edge_attr = torch.from_numpy(val_edge_attr).float().to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "        train_losses = []\n",
        "        val_metrics = {'auc': [], 'ap': [], 'loss': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            z, edge_features = model(data.x, train_edge_index, train_edge_attr)\n",
        "            pos_out = model.decode(z, edge_features, train_edge_index)\n",
        "\n",
        "            # Generate negative samples\n",
        "            neg_edge_index = negative_sampling(\n",
        "                train_edge_index,\n",
        "                num_nodes=data.num_nodes,\n",
        "                num_neg_samples=train_edge_index.size(1)\n",
        "            )\n",
        "\n",
        "            neg_out = model.decode(z, edge_features, neg_edge_index)\n",
        "            loss = bce_with_logits_loss(pos_out, neg_out)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                z, edge_features = model(data.x, val_edge_index, val_edge_attr)\n",
        "                pos_out = model.decode(z, edge_features, val_edge_index)\n",
        "\n",
        "                neg_edge_index = negative_sampling(\n",
        "                    val_edge_index,\n",
        "                    num_nodes=data.num_nodes,\n",
        "                    num_neg_samples=val_edge_index.size(1)\n",
        "                )\n",
        "\n",
        "                neg_out = model.decode(z, edge_features, neg_edge_index)\n",
        "                val_loss = bce_with_logits_loss(pos_out, neg_out)\n",
        "\n",
        "                # Compute metrics\n",
        "                pred = torch.cat([pos_out, neg_out]).cpu().numpy()\n",
        "                true = torch.cat([\n",
        "                    torch.ones(pos_out.size(0)),\n",
        "                    torch.zeros(neg_out.size(0))\n",
        "                ]).numpy()\n",
        "\n",
        "                auc = roc_auc_score(true, pred)\n",
        "                ap = average_precision_score(true, pred)\n",
        "\n",
        "                val_metrics['auc'].append(auc)\n",
        "                val_metrics['ap'].append(ap)\n",
        "                val_metrics['loss'].append(val_loss.item())\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f\"Epoch {epoch + 1}: Train Loss = {loss:.4f}, \"\n",
        "                          f\"Val Loss = {val_loss:.4f}, AUC = {auc:.4f}, AP = {ap:.4f}\")\n",
        "\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    early_stopping_counter = 0\n",
        "                else:\n",
        "                    early_stopping_counter += 1\n",
        "\n",
        "                if early_stopping_counter >= 20:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        fold_results = {\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_metrics': val_metrics,\n",
        "            'final_auc': auc,\n",
        "            'final_ap': ap\n",
        "        }\n",
        "        all_results.append(fold_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def visualize_results(results, network_data, save_path='qkd_results'):\n",
        "    \"\"\"Create comprehensive visualizations and analysis\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    save_path = f\"{save_path}_{timestamp}\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Find the minimum length across all result arrays\n",
        "    min_epochs = min(len(result['train_losses']) for result in results)\n",
        "\n",
        "    # Truncate all arrays to the minimum length\n",
        "    truncated_results = []\n",
        "    for result in results:\n",
        "        truncated_result = {\n",
        "            'train_losses': result['train_losses'][:min_epochs],\n",
        "            'val_metrics': {\n",
        "                'auc': result['val_metrics']['auc'][:min_epochs],\n",
        "                'ap': result['val_metrics']['ap'][:min_epochs],\n",
        "                'loss': result['val_metrics']['loss'][:min_epochs]\n",
        "            },\n",
        "            'final_auc': result['final_auc'],\n",
        "            'final_ap': result['final_ap']\n",
        "        }\n",
        "        truncated_results.append(truncated_result)\n",
        "\n",
        "    # Training Metrics\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot training losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    for result in truncated_results:\n",
        "        plt.plot(result['train_losses'], alpha=0.3)\n",
        "    mean_train_loss = np.mean([r['train_losses'] for r in truncated_results], axis=0)\n",
        "    plt.plot(mean_train_loss, 'r-', label='Mean')\n",
        "    plt.title('Training Loss Evolution')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot validation AUC\n",
        "    plt.subplot(2, 2, 2)\n",
        "    for result in truncated_results:\n",
        "        plt.plot(result['val_metrics']['auc'], alpha=0.3)\n",
        "    mean_val_auc = np.mean([r['val_metrics']['auc'] for r in truncated_results], axis=0)\n",
        "    plt.plot(mean_val_auc, 'r-', label='Mean')\n",
        "    plt.title('Validation AUC Evolution')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    # Key Rate vs Distance Analysis\n",
        "    distances = network_data.edge_attr[:, 2].cpu().numpy()\n",
        "    key_rates = network_data.edge_attr[:, 0].cpu().numpy()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.scatter(distances, key_rates, alpha=0.5)\n",
        "    plt.xlabel('Distance (km)')\n",
        "    plt.ylabel('Key Rate (bits/s)')\n",
        "    plt.yscale('log')\n",
        "    plt.title('Key Rate vs Distance')\n",
        "\n",
        "    x_fit = np.linspace(min(distances), max(distances), 100)\n",
        "    y_fit = np.exp(-0.2 * x_fit)\n",
        "    plt.plot(x_fit, y_fit * max(key_rates), 'r--', label='Theoretical')\n",
        "    plt.legend()\n",
        "\n",
        "    # QBER Distribution\n",
        "    plt.subplot(2, 2, 4)\n",
        "    qber_values = network_data.edge_attr[:, 1].cpu().numpy()\n",
        "    sns.histplot(qber_values, bins=20)\n",
        "    plt.xlabel('QBER')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('QBER Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/training_metrics.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Performance Report\n",
        "    report = {\n",
        "        'network_stats': {\n",
        "            'num_nodes': int(network_data.num_nodes),\n",
        "            'num_edges': int(len(key_rates)),\n",
        "            'avg_degree': float(2 * len(key_rates) / network_data.num_nodes),\n",
        "            'avg_key_rate': float(np.mean(key_rates)),\n",
        "            'avg_qber': float(np.mean(qber_values)),\n",
        "            'max_distance': float(np.max(distances))\n",
        "        },\n",
        "        'model_performance': {\n",
        "            'final_metrics': {\n",
        "                'auc_mean': float(np.mean([r['final_auc'] for r in results])),\n",
        "                'auc_std': float(np.std([r['final_auc'] for r in results])),\n",
        "                'ap_mean': float(np.mean([r['final_ap'] for r in results])),\n",
        "                'ap_std': float(np.std([r['final_ap'] for r in results]))\n",
        "            },\n",
        "            'convergence': {\n",
        "                'final_train_loss_mean': float(np.mean([r['train_losses'][-1] for r in truncated_results])),\n",
        "                'best_epoch_mean': float(np.mean([np.argmin(r['val_metrics']['loss']) for r in truncated_results]))\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f\"{save_path}/performance_report.json\", 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "\n",
        "    return report\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate network\n",
        "    print(\"Generating QKD network...\")\n",
        "    network = AdvancedQKDNetwork(num_nodes=50)\n",
        "    data = network.generate_graph_data()\n",
        "\n",
        "    # Create and train model\n",
        "    print(\"Creating and training model...\")\n",
        "    model = AdvancedQKDLinkPredictor(\n",
        "        in_channels=data.x.size(1),\n",
        "        edge_attr_channels=data.edge_attr.size(1)\n",
        "    )\n",
        "\n",
        "    # Train and evaluate\n",
        "    results = train_and_evaluate(model, data)\n",
        "\n",
        "    # Generate visualizations and analysis\n",
        "    print(\"Generating visualizations and analysis...\")\n",
        "    save_path = 'qkd_results'\n",
        "    performance_report = visualize_results(results, data, save_path)\n",
        "\n",
        "    print(f\"\\nAnalysis complete. Results saved in: {save_path}\")\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.edge_index.size(1) // 2}\")\n",
        "    print(f\"Average degree: {data.edge_index.size(1) / data.num_nodes:.2f}\")\n",
        "    print(f\"Average key rate: {performance_report['network_stats']['avg_key_rate']:.2e} bits/s\")\n",
        "    print(f\"Average QBER: {performance_report['network_stats']['avg_qber']:.3f}\")\n",
        "    print(f\"Model AUC: {performance_report['model_performance']['final_metrics']['auc_mean']:.3f} ± \"\n",
        "          f\"{performance_report['model_performance']['final_metrics']['auc_std']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PNr8G5z1Go6Y",
        "outputId": "41c30050-7d99-4296-db68-dbf62429aea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating QKD network...\n",
            "Creating GNN model...\n",
            "Running comprehensive evaluation...\n",
            "Using device: cpu\n",
            "Running comprehensive evaluation with classical baselines...\n",
            "Found 199 unique edges\n",
            "\n",
            "Fold 1/3\n",
            "Training GNN model...\n",
            "  GNN Epoch 0: Loss = 1.5664\n",
            "  GNN Epoch 10: Loss = 1.1257\n",
            "  GNN Epoch 20: Loss = 0.8925\n",
            "  GNN Epoch 30: Loss = 0.7775\n",
            "  GNN Epoch 40: Loss = 0.7464\n",
            "Evaluating all methods...\n",
            "  Evaluating GNN...\n",
            "  Evaluating classical routing methods...\n",
            "  Evaluating ML baselines...\n",
            "  Evaluating Genetic Algorithm...\n",
            "\n",
            "  Fold 1 Results:\n",
            "    GNN: AUC = 0.8492, AP = 0.7965\n",
            "    Dijkstra: AUC = 0.3416, AP = 0.4075\n",
            "    MST: AUC = 0.3617, AP = 0.4235\n",
            "    Greedy: AUC = 0.4745, AP = 0.4675\n",
            "    Random Forest: AUC = 0.5000, AP = 0.5000\n",
            "    Linear Regression: AUC = 0.5000, AP = 0.5000\n",
            "    Genetic Algorithm: AUC = 0.5124, AP = 0.5096\n",
            "\n",
            "Fold 2/3\n",
            "Training GNN model...\n",
            "  GNN Epoch 0: Loss = 0.9656\n",
            "  GNN Epoch 10: Loss = 0.7123\n",
            "  GNN Epoch 20: Loss = 0.6587\n",
            "  GNN Epoch 30: Loss = 0.7420\n",
            "  GNN Epoch 40: Loss = 0.6048\n",
            "Evaluating all methods...\n",
            "  Evaluating GNN...\n",
            "  Evaluating classical routing methods...\n",
            "  Evaluating ML baselines...\n",
            "  Evaluating Genetic Algorithm...\n",
            "\n",
            "  Fold 2 Results:\n",
            "    GNN: AUC = 0.7505, AP = 0.7101\n",
            "    Dijkstra: AUC = 0.5674, AP = 0.5549\n",
            "    MST: AUC = 0.5806, AP = 0.5854\n",
            "    Greedy: AUC = 0.4679, AP = 0.5379\n",
            "    Random Forest: AUC = 0.5000, AP = 0.5000\n",
            "    Linear Regression: AUC = 0.5000, AP = 0.5000\n",
            "    Genetic Algorithm: AUC = 0.5390, AP = 0.5779\n",
            "\n",
            "Fold 3/3\n",
            "Training GNN model...\n",
            "  GNN Epoch 0: Loss = 0.7628\n",
            "  GNN Epoch 10: Loss = 0.6254\n",
            "  GNN Epoch 20: Loss = 0.5768\n",
            "  GNN Epoch 30: Loss = 0.6413\n",
            "  GNN Epoch 40: Loss = 0.5238\n",
            "Evaluating all methods...\n",
            "  Evaluating GNN...\n",
            "  Evaluating classical routing methods...\n",
            "  Evaluating ML baselines...\n",
            "  Evaluating Genetic Algorithm...\n",
            "\n",
            "  Fold 3 Results:\n",
            "    GNN: AUC = 0.8386, AP = 0.8083\n",
            "    Dijkstra: AUC = 0.3557, AP = 0.4087\n",
            "    MST: AUC = 0.4092, AP = 0.4528\n",
            "    Greedy: AUC = 0.5349, AP = 0.4898\n",
            "    Random Forest: AUC = 0.5000, AP = 0.5000\n",
            "    Linear Regression: AUC = 0.5000, AP = 0.5000\n",
            "    Genetic Algorithm: AUC = 0.5680, AP = 0.5547\n",
            "Generating comparative analysis...\n",
            "\n",
            "============================================================\n",
            "COMPARATIVE ANALYSIS SUMMARY\n",
            "============================================================\n",
            "Best AUC: GNN (0.8128)\n",
            "Best AP:  GNN (0.7717)\n",
            "GNN Rank (AUC): 1/7\n",
            "GNN Rank (AP):  1/7\n",
            "\n",
            "GNN Improvements over Classical Methods:\n",
            "  vs Dijkstra:\n",
            "    AUC: +92.79%\n",
            "    AP:  +68.84%\n",
            "  vs MST:\n",
            "    AUC: +80.42%\n",
            "    AP:  +58.37%\n",
            "  vs Greedy:\n",
            "    AUC: +65.05%\n",
            "    AP:  +54.83%\n",
            "  vs Random Forest:\n",
            "    AUC: +62.55%\n",
            "    AP:  +54.33%\n",
            "  vs Linear Regression:\n",
            "    AUC: +62.55%\n",
            "    AP:  +54.33%\n",
            "  vs Genetic Algorithm:\n",
            "    AUC: +50.57%\n",
            "    AP:  +40.96%\n",
            "\n",
            "Statistical Significance (p < 0.05):\n",
            "  vs Dijkstra:\n",
            "    AUC: Not significant (p=0.0643)\n",
            "    AP:  Not significant (p=0.0586)\n",
            "  vs MST:\n",
            "    AUC: Not significant (p=0.0656)\n",
            "    AP:  Not significant (p=0.0708)\n",
            "  vs Greedy:\n",
            "    AUC: Significant (p=0.0075)\n",
            "    AP:  Significant (p=0.0326)\n",
            "  vs Random Forest:\n",
            "    AUC: Significant (p=0.0099)\n",
            "    AP:  Significant (p=0.0127)\n",
            "  vs Linear Regression:\n",
            "    AUC: Significant (p=0.0099)\n",
            "    AP:  Significant (p=0.0127)\n",
            "  vs Genetic Algorithm:\n",
            "    AUC: Significant (p=0.0172)\n",
            "    AP:  Significant (p=0.0413)\n",
            "\n",
            "Comparative analysis complete!\n",
            "Results demonstrate the relative performance of GNN vs classical methods.\n"
          ]
        }
      ],
      "source": [
        "# Add these imports to the existing imports cell\n",
        "import heapq\n",
        "from scipy.optimize import differential_evolution\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ClassicalRoutingMethods:\n",
        "    \"\"\"Classical routing and optimization methods for QKD networks\"\"\"\n",
        "\n",
        "    def __init__(self, network_data):\n",
        "        self.data = network_data\n",
        "        self.G = self._build_networkx_graph()\n",
        "\n",
        "    def _build_networkx_graph(self):\n",
        "        \"\"\"Build NetworkX graph from PyTorch Geometric data\"\"\"\n",
        "        G = nx.Graph()\n",
        "        edge_index = self.data.edge_index.cpu().numpy()\n",
        "        edge_attr = self.data.edge_attr.cpu().numpy()\n",
        "\n",
        "        # Handle the case where edges are duplicated in edge_index but not in edge_attr\n",
        "        # Create a mapping from edge to attributes\n",
        "        edge_to_attr = {}\n",
        "\n",
        "        # Since we use to_undirected, edges are duplicated in edge_index\n",
        "        # But edge_attr contains only unique edges\n",
        "        num_unique_edges = edge_attr.shape[0]\n",
        "\n",
        "        for i in range(num_unique_edges):\n",
        "            # For each unique edge, we need to find its position in the original edge_index\n",
        "            # Since to_undirected creates [src,dst] and [dst,src], we handle both\n",
        "            if i < edge_index.shape[1] // 2:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            else:\n",
        "                # For the second half, edges are reversed\n",
        "                original_idx = i - edge_index.shape[1] // 2\n",
        "                dst, src = edge_index[0, original_idx], edge_index[1, original_idx]\n",
        "\n",
        "            edge_key = tuple(sorted([src, dst]))\n",
        "            if edge_key not in edge_to_attr:\n",
        "                key_rate = edge_attr[i % num_unique_edges, 0]\n",
        "                qber = edge_attr[i % num_unique_edges, 1]\n",
        "                distance = edge_attr[i % num_unique_edges, 2]\n",
        "                edge_to_attr[edge_key] = (key_rate, qber, distance)\n",
        "\n",
        "        # Now build the graph with unique edges\n",
        "        for edge_key, (key_rate, qber, distance) in edge_to_attr.items():\n",
        "            src, dst = edge_key\n",
        "\n",
        "            # Use negative key rate as weight for shortest path algorithms\n",
        "            weight = 1.0 / (key_rate + 1e-10)  # Avoid division by zero\n",
        "\n",
        "            G.add_edge(src, dst,\n",
        "                      weight=weight,\n",
        "                      key_rate=key_rate,\n",
        "                      qber=qber,\n",
        "                      distance=distance)\n",
        "\n",
        "        return G\n",
        "\n",
        "    def dijkstra_routing(self, source_target_pairs):\n",
        "        \"\"\"Dijkstra's shortest path algorithm\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for src, dst in source_target_pairs:\n",
        "            try:\n",
        "                # Find shortest path based on inverse key rate\n",
        "                path = nx.shortest_path(self.G, src, dst, weight='weight')\n",
        "                path_exists = len(path) > 1\n",
        "\n",
        "                if path_exists:\n",
        "                    # Calculate path quality metrics\n",
        "                    total_key_rate = 1.0\n",
        "                    max_qber = 0.0\n",
        "                    total_distance = 0.0\n",
        "\n",
        "                    for i in range(len(path) - 1):\n",
        "                        edge_data = self.G[path[i]][path[i+1]]\n",
        "                        total_key_rate *= edge_data['key_rate']\n",
        "                        max_qber = max(max_qber, edge_data['qber'])\n",
        "                        total_distance += edge_data['distance']\n",
        "\n",
        "                    # Score based on end-to-end key rate\n",
        "                    score = total_key_rate if max_qber < 0.11 else 0.0\n",
        "                else:\n",
        "                    score = 0.0\n",
        "\n",
        "            except nx.NetworkXNoPath:\n",
        "                score = 0.0\n",
        "            except KeyError:\n",
        "                score = 0.0\n",
        "\n",
        "            predictions.append(score)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def minimum_spanning_tree(self, source_target_pairs):\n",
        "        \"\"\"MST-based routing\"\"\"\n",
        "        try:\n",
        "            mst = nx.minimum_spanning_tree(self.G, weight='weight')\n",
        "        except:\n",
        "            # If MST fails, use original graph\n",
        "            mst = self.G\n",
        "\n",
        "        predictions = []\n",
        "        for src, dst in source_target_pairs:\n",
        "            try:\n",
        "                path = nx.shortest_path(mst, src, dst)\n",
        "                if len(path) > 1:\n",
        "                    # Calculate path quality\n",
        "                    total_key_rate = 1.0\n",
        "                    for i in range(len(path) - 1):\n",
        "                        if mst.has_edge(path[i], path[i+1]):\n",
        "                            edge_data = mst[path[i]][path[i+1]]\n",
        "                            total_key_rate *= edge_data.get('key_rate', 0.001)\n",
        "                    score = total_key_rate\n",
        "                else:\n",
        "                    score = 0.0\n",
        "            except (nx.NetworkXNoPath, KeyError):\n",
        "                score = 0.0\n",
        "            predictions.append(score)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def greedy_best_first(self, source_target_pairs):\n",
        "        \"\"\"Greedy best-first search based on key rate\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for src, dst in source_target_pairs:\n",
        "            try:\n",
        "                visited = set()\n",
        "                current = src\n",
        "                path_key_rate = 1.0\n",
        "                found_path = False\n",
        "                max_hops = 10  # Prevent infinite loops\n",
        "\n",
        "                while current != dst and current not in visited and len(visited) < max_hops:\n",
        "                    visited.add(current)\n",
        "\n",
        "                    # Find best next hop based on key rate\n",
        "                    best_neighbor = None\n",
        "                    best_key_rate = 0.0\n",
        "\n",
        "                    if current in self.G:\n",
        "                        for neighbor in self.G.neighbors(current):\n",
        "                            if neighbor not in visited:\n",
        "                                edge_data = self.G[current][neighbor]\n",
        "                                if edge_data['key_rate'] > best_key_rate:\n",
        "                                    best_key_rate = edge_data['key_rate']\n",
        "                                    best_neighbor = neighbor\n",
        "\n",
        "                    if best_neighbor is None:\n",
        "                        break\n",
        "\n",
        "                    path_key_rate *= best_key_rate\n",
        "                    current = best_neighbor\n",
        "\n",
        "                    if current == dst:\n",
        "                        found_path = True\n",
        "                        break\n",
        "\n",
        "                score = path_key_rate if found_path else 0.0\n",
        "            except:\n",
        "                score = 0.0\n",
        "\n",
        "            predictions.append(score)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "class GeneticAlgorithmOptimizer:\n",
        "    \"\"\"Genetic Algorithm for QKD network optimization\"\"\"\n",
        "\n",
        "    def __init__(self, network_data, population_size=50, generations=100):\n",
        "        self.data = network_data\n",
        "        self.population_size = population_size\n",
        "        self.generations = generations\n",
        "        self.num_nodes = network_data.num_nodes\n",
        "\n",
        "        # Build adjacency matrix for faster access\n",
        "        self.adj_matrix = self._build_adjacency_matrix()\n",
        "\n",
        "    def _build_adjacency_matrix(self):\n",
        "        \"\"\"Build adjacency matrix with key rates\"\"\"\n",
        "        adj = np.zeros((self.num_nodes, self.num_nodes))\n",
        "        edge_index = self.data.edge_index.cpu().numpy()\n",
        "        edge_attr = self.data.edge_attr.cpu().numpy()\n",
        "\n",
        "        # Handle undirected edges correctly\n",
        "        num_unique_edges = edge_attr.shape[0]\n",
        "\n",
        "        for i in range(min(edge_index.shape[1], num_unique_edges)):\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            key_rate = edge_attr[i, 0]\n",
        "            adj[src, dst] = key_rate\n",
        "            adj[dst, src] = key_rate\n",
        "\n",
        "        return adj\n",
        "\n",
        "    def _fitness_function(self, individual, source_target_pairs):\n",
        "        \"\"\"Evaluate fitness of an individual (routing solution)\"\"\"\n",
        "        total_score = 0.0\n",
        "\n",
        "        for i, (src, dst) in enumerate(source_target_pairs):\n",
        "            # Individual encodes path selection probabilities\n",
        "            path_score = self._evaluate_path(src, dst, individual)\n",
        "            total_score += path_score\n",
        "\n",
        "        return total_score / max(len(source_target_pairs), 1)\n",
        "\n",
        "    def _evaluate_path(self, src, dst, individual):\n",
        "        \"\"\"Evaluate a specific path using individual's strategy\"\"\"\n",
        "        current = src\n",
        "        visited = set()\n",
        "        path_key_rate = 1.0\n",
        "\n",
        "        while current != dst and current not in visited and len(visited) < 10:\n",
        "            visited.add(current)\n",
        "\n",
        "            # Use individual's weights to select next hop\n",
        "            neighbors = np.where(self.adj_matrix[current] > 0)[0]\n",
        "            neighbors = [n for n in neighbors if n not in visited]\n",
        "\n",
        "            if not neighbors:\n",
        "                return 0.0\n",
        "\n",
        "            # Weight neighbors based on individual's preferences\n",
        "            weights = []\n",
        "            for neighbor in neighbors:\n",
        "                key_rate = self.adj_matrix[current, neighbor]\n",
        "                # Individual influences neighbor selection\n",
        "                weight = key_rate * (1 + individual[neighbor % len(individual)])\n",
        "                weights.append(weight)\n",
        "\n",
        "            if not weights:\n",
        "                return 0.0\n",
        "\n",
        "            # Select best neighbor\n",
        "            best_idx = np.argmax(weights)\n",
        "            next_node = neighbors[best_idx]\n",
        "            path_key_rate *= self.adj_matrix[current, next_node]\n",
        "            current = next_node\n",
        "\n",
        "        return path_key_rate if current == dst else 0.0\n",
        "\n",
        "    def optimize(self, source_target_pairs):\n",
        "        \"\"\"Run genetic algorithm optimization\"\"\"\n",
        "        if not source_target_pairs:\n",
        "            return np.array([])\n",
        "\n",
        "        # Initialize population\n",
        "        population = []\n",
        "        for _ in range(self.population_size):\n",
        "            individual = np.random.uniform(-1, 1, self.num_nodes)\n",
        "            population.append(individual)\n",
        "\n",
        "        best_fitness = -float('inf')\n",
        "        best_individual = None\n",
        "\n",
        "        for generation in range(self.generations):\n",
        "            # Evaluate fitness\n",
        "            fitness_scores = []\n",
        "            for individual in population:\n",
        "                fitness = self._fitness_function(individual, source_target_pairs)\n",
        "                fitness_scores.append(fitness)\n",
        "\n",
        "                if fitness > best_fitness:\n",
        "                    best_fitness = fitness\n",
        "                    best_individual = individual.copy()\n",
        "\n",
        "            # Selection and reproduction\n",
        "            new_population = []\n",
        "\n",
        "            # Elitism: keep best individuals\n",
        "            elite_size = max(1, self.population_size // 10)\n",
        "            elite_indices = np.argsort(fitness_scores)[-elite_size:]\n",
        "            for idx in elite_indices:\n",
        "                new_population.append(population[idx].copy())\n",
        "\n",
        "            # Crossover and mutation\n",
        "            while len(new_population) < self.population_size:\n",
        "                # Tournament selection\n",
        "                parent1 = self._tournament_selection(population, fitness_scores)\n",
        "                parent2 = self._tournament_selection(population, fitness_scores)\n",
        "\n",
        "                # Crossover\n",
        "                child = self._crossover(parent1, parent2)\n",
        "\n",
        "                # Mutation\n",
        "                child = self._mutate(child)\n",
        "\n",
        "                new_population.append(child)\n",
        "\n",
        "            population = new_population\n",
        "\n",
        "        # Generate predictions using best individual\n",
        "        if best_individual is None:\n",
        "            return np.zeros(len(source_target_pairs))\n",
        "\n",
        "        predictions = []\n",
        "        for src, dst in source_target_pairs:\n",
        "            score = self._evaluate_path(src, dst, best_individual)\n",
        "            predictions.append(score)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _tournament_selection(self, population, fitness_scores, tournament_size=3):\n",
        "        \"\"\"Tournament selection for parent selection\"\"\"\n",
        "        tournament_size = min(tournament_size, len(population))\n",
        "        tournament_indices = np.random.choice(len(population), tournament_size, replace=False)\n",
        "        tournament_fitness = [fitness_scores[i] for i in tournament_indices]\n",
        "        winner_idx = tournament_indices[np.argmax(tournament_fitness)]\n",
        "        return population[winner_idx].copy()\n",
        "\n",
        "    def _crossover(self, parent1, parent2):\n",
        "        \"\"\"Single-point crossover\"\"\"\n",
        "        crossover_point = np.random.randint(1, len(parent1))\n",
        "        child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "        return child\n",
        "\n",
        "    def _mutate(self, individual, mutation_rate=0.1):\n",
        "        \"\"\"Gaussian mutation\"\"\"\n",
        "        mask = np.random.random(len(individual)) < mutation_rate\n",
        "        individual[mask] += np.random.normal(0, 0.1, np.sum(mask))\n",
        "        return np.clip(individual, -1, 1)\n",
        "\n",
        "class MachineLearningBaselines:\n",
        "    \"\"\"Classical ML approaches for link prediction\"\"\"\n",
        "\n",
        "    def __init__(self, network_data):\n",
        "        self.data = network_data\n",
        "\n",
        "    def random_forest_predictor(self, train_pairs, test_pairs, train_labels, test_labels):\n",
        "        \"\"\"Random Forest baseline\"\"\"\n",
        "        # Extract features for training\n",
        "        train_features = self._extract_features(train_pairs)\n",
        "        test_features = self._extract_features(test_pairs)\n",
        "\n",
        "        if train_features.shape[0] == 0 or test_features.shape[0] == 0:\n",
        "            return np.zeros(len(test_pairs))\n",
        "\n",
        "        # Train Random Forest\n",
        "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf.fit(train_features, train_labels)\n",
        "\n",
        "        # Predict\n",
        "        predictions = rf.predict(test_features)\n",
        "        return predictions\n",
        "\n",
        "    def linear_regression_predictor(self, train_pairs, test_pairs, train_labels, test_labels):\n",
        "        \"\"\"Linear Regression baseline\"\"\"\n",
        "        train_features = self._extract_features(train_pairs)\n",
        "        test_features = self._extract_features(test_pairs)\n",
        "\n",
        "        if train_features.shape[0] == 0 or test_features.shape[0] == 0:\n",
        "            return np.zeros(len(test_pairs))\n",
        "\n",
        "        lr = LinearRegression()\n",
        "        lr.fit(train_features, train_labels)\n",
        "\n",
        "        predictions = lr.predict(test_features)\n",
        "        return predictions\n",
        "\n",
        "    def _extract_features(self, node_pairs):\n",
        "        \"\"\"Extract features for node pairs\"\"\"\n",
        "        if not node_pairs:\n",
        "            return np.array([]).reshape(0, 10)  # Return empty array with correct shape\n",
        "\n",
        "        features = []\n",
        "        node_features = self.data.x.cpu().numpy()\n",
        "        edge_index = self.data.edge_index.cpu().numpy()\n",
        "\n",
        "        # Build adjacency list for faster lookup\n",
        "        adj_list = {}\n",
        "        num_unique_edges = min(edge_index.shape[1], self.data.edge_attr.shape[0])\n",
        "\n",
        "        for i in range(num_unique_edges):\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            if src not in adj_list:\n",
        "                adj_list[src] = []\n",
        "            if dst not in adj_list:\n",
        "                adj_list[dst] = []\n",
        "            adj_list[src].append(dst)\n",
        "            adj_list[dst].append(src)\n",
        "\n",
        "        for src, dst in node_pairs:\n",
        "            # Node features\n",
        "            src_features = node_features[src] if src < len(node_features) else np.zeros(4)\n",
        "            dst_features = node_features[dst] if dst < len(node_features) else np.zeros(4)\n",
        "\n",
        "            # Distance between nodes\n",
        "            distance = np.linalg.norm(src_features[:2] - dst_features[:2])\n",
        "\n",
        "            # Common neighbors\n",
        "            src_neighbors = set(adj_list.get(src, []))\n",
        "            dst_neighbors = set(adj_list.get(dst, []))\n",
        "            common_neighbors = len(src_neighbors.intersection(dst_neighbors))\n",
        "\n",
        "            # Combine features\n",
        "            pair_features = np.concatenate([\n",
        "                src_features,\n",
        "                dst_features,\n",
        "                [distance, common_neighbors]\n",
        "            ])\n",
        "\n",
        "            features.append(pair_features)\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "def comprehensive_evaluation(model, data, num_epochs=200, k_folds=5):\n",
        "    \"\"\"Comprehensive evaluation comparing GNN with classical methods\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Running comprehensive evaluation with classical baselines...\")\n",
        "\n",
        "    # Initialize classical methods\n",
        "    classical_routing = ClassicalRoutingMethods(data)\n",
        "    ml_baselines = MachineLearningBaselines(data)\n",
        "\n",
        "    all_results = []\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "    # Prepare edge data - handle the undirected edge issue\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    edge_attr = data.edge_attr.cpu().numpy()\n",
        "\n",
        "    # Create unique edges from the edge_index\n",
        "    unique_edges = set()\n",
        "    edge_to_idx = {}\n",
        "\n",
        "    # Since edges are undirected, we only take the first half\n",
        "    num_unique_edges = edge_attr.shape[0]\n",
        "\n",
        "    for i in range(num_unique_edges):\n",
        "        if i < edge_index.shape[1]:\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            edge = tuple(sorted([src, dst]))\n",
        "            if edge not in unique_edges:\n",
        "                unique_edges.add(edge)\n",
        "                edge_to_idx[edge] = i\n",
        "\n",
        "    unique_edges = list(unique_edges)\n",
        "    print(f\"Found {len(unique_edges)} unique edges\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(unique_edges)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "\n",
        "        train_edges = [unique_edges[i] for i in train_idx]\n",
        "        val_edges = [unique_edges[i] for i in val_idx]\n",
        "\n",
        "        # Prepare training data\n",
        "        train_edge_list = []\n",
        "        train_attr_list = []\n",
        "\n",
        "        for edge in train_edges:\n",
        "            train_edge_list.append([edge[0], edge[1]])\n",
        "            attr_idx = edge_to_idx[edge]\n",
        "            train_attr_list.append(edge_attr[attr_idx])\n",
        "\n",
        "        val_edge_list = []\n",
        "        val_attr_list = []\n",
        "\n",
        "        for edge in val_edges:\n",
        "            val_edge_list.append([edge[0], edge[1]])\n",
        "            attr_idx = edge_to_idx[edge]\n",
        "            val_attr_list.append(edge_attr[attr_idx])\n",
        "\n",
        "        if not train_edge_list or not val_edge_list:\n",
        "            print(f\"  Skipping fold {fold + 1} due to empty train or validation set\")\n",
        "            continue\n",
        "\n",
        "        train_edge_index = torch.tensor(train_edge_list, dtype=torch.long).t().to(device)\n",
        "        train_edge_attr = torch.tensor(train_attr_list, dtype=torch.float).to(device)\n",
        "        val_edge_index = torch.tensor(val_edge_list, dtype=torch.long).t().to(device)\n",
        "        val_edge_attr = torch.tensor(val_attr_list, dtype=torch.float).to(device)\n",
        "\n",
        "        # Train GNN model\n",
        "        print(\"Training GNN model...\")\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "        # Simplified training loop for comparison\n",
        "        for epoch in range(min(num_epochs, 50)):  # Reduced for faster comparison\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            z, edge_features = model(data.x, train_edge_index, train_edge_attr)\n",
        "            pos_out = model.decode(z, edge_features, train_edge_index)\n",
        "\n",
        "            neg_edge_index = negative_sampling(\n",
        "                train_edge_index,\n",
        "                num_nodes=data.num_nodes,\n",
        "                num_neg_samples=train_edge_index.size(1)\n",
        "            )\n",
        "\n",
        "            neg_out = model.decode(z, edge_features, neg_edge_index)\n",
        "            loss = bce_with_logits_loss(pos_out, neg_out)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"  GNN Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "\n",
        "        # Evaluate all methods\n",
        "        print(\"Evaluating all methods...\")\n",
        "\n",
        "        # Prepare test data\n",
        "        val_pairs = [(val_edge_index[0, i].item(), val_edge_index[1, i].item())\n",
        "                     for i in range(val_edge_index.size(1))]\n",
        "        train_pairs = [(train_edge_index[0, i].item(), train_edge_index[1, i].item())\n",
        "                       for i in range(train_edge_index.size(1))]\n",
        "\n",
        "        # Generate negative samples for fair comparison\n",
        "        val_neg_edge_index = negative_sampling(\n",
        "            val_edge_index,\n",
        "            num_nodes=data.num_nodes,\n",
        "            num_neg_samples=val_edge_index.size(1)\n",
        "        )\n",
        "        val_neg_pairs = [(val_neg_edge_index[0, i].item(), val_neg_edge_index[1, i].item())\n",
        "                         for i in range(val_neg_edge_index.size(1))]\n",
        "\n",
        "        all_val_pairs = val_pairs + val_neg_pairs\n",
        "        true_labels = [1] * len(val_pairs) + [0] * len(val_neg_pairs)\n",
        "\n",
        "        train_labels = [1] * len(train_pairs)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # 1. GNN Evaluation\n",
        "        print(\"  Evaluating GNN...\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            z, edge_features = model(data.x, val_edge_index, val_edge_attr)\n",
        "            pos_out = model.decode(z, edge_features, val_edge_index)\n",
        "            neg_out = model.decode(z, edge_features, val_neg_edge_index)\n",
        "\n",
        "            gnn_pred = torch.cat([pos_out, neg_out]).cpu().numpy()\n",
        "            gnn_auc = roc_auc_score(true_labels, gnn_pred)\n",
        "            gnn_ap = average_precision_score(true_labels, gnn_pred)\n",
        "\n",
        "        results['GNN'] = {\n",
        "            'auc': gnn_auc,\n",
        "            'ap': gnn_ap,\n",
        "            'predictions': gnn_pred\n",
        "        }\n",
        "\n",
        "        # 2. Classical Routing Methods\n",
        "        print(\"  Evaluating classical routing methods...\")\n",
        "\n",
        "        # Dijkstra\n",
        "        start_time = time.time()\n",
        "        dijkstra_pred = classical_routing.dijkstra_routing(all_val_pairs)\n",
        "        dijkstra_time = time.time() - start_time\n",
        "\n",
        "        # Normalize predictions for fair comparison\n",
        "        if np.max(dijkstra_pred) > 0:\n",
        "            dijkstra_pred = dijkstra_pred / np.max(dijkstra_pred)\n",
        "\n",
        "        dijkstra_auc = roc_auc_score(true_labels, dijkstra_pred) if len(np.unique(dijkstra_pred)) > 1 else 0.5\n",
        "        dijkstra_ap = average_precision_score(true_labels, dijkstra_pred)\n",
        "\n",
        "        results['Dijkstra'] = {\n",
        "            'auc': dijkstra_auc,\n",
        "            'ap': dijkstra_ap,\n",
        "            'time': dijkstra_time,\n",
        "            'predictions': dijkstra_pred\n",
        "        }\n",
        "\n",
        "        # MST\n",
        "        start_time = time.time()\n",
        "        mst_pred = classical_routing.minimum_spanning_tree(all_val_pairs)\n",
        "        mst_time = time.time() - start_time\n",
        "\n",
        "        if np.max(mst_pred) > 0:\n",
        "            mst_pred = mst_pred / np.max(mst_pred)\n",
        "\n",
        "        mst_auc = roc_auc_score(true_labels, mst_pred) if len(np.unique(mst_pred)) > 1 else 0.5\n",
        "        mst_ap = average_precision_score(true_labels, mst_pred)\n",
        "\n",
        "        results['MST'] = {\n",
        "            'auc': mst_auc,\n",
        "            'ap': mst_ap,\n",
        "            'time': mst_time,\n",
        "            'predictions': mst_pred\n",
        "        }\n",
        "\n",
        "        # Greedy Best-First\n",
        "        start_time = time.time()\n",
        "        greedy_pred = classical_routing.greedy_best_first(all_val_pairs)\n",
        "        greedy_time = time.time() - start_time\n",
        "\n",
        "        if np.max(greedy_pred) > 0:\n",
        "            greedy_pred = greedy_pred / np.max(greedy_pred)\n",
        "\n",
        "        greedy_auc = roc_auc_score(true_labels, greedy_pred) if len(np.unique(greedy_pred)) > 1 else 0.5\n",
        "        greedy_ap = average_precision_score(true_labels, greedy_pred)\n",
        "\n",
        "        results['Greedy'] = {\n",
        "            'auc': greedy_auc,\n",
        "            'ap': greedy_ap,\n",
        "            'time': greedy_time,\n",
        "            'predictions': greedy_pred\n",
        "        }\n",
        "\n",
        "        # 3. Machine Learning Baselines\n",
        "        print(\"  Evaluating ML baselines...\")\n",
        "\n",
        "        # Random Forest\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            rf_pred = ml_baselines.random_forest_predictor(\n",
        "                train_pairs, all_val_pairs, train_labels, true_labels\n",
        "            )\n",
        "            rf_time = time.time() - start_time\n",
        "\n",
        "            rf_auc = roc_auc_score(true_labels, rf_pred)\n",
        "            rf_ap = average_precision_score(true_labels, rf_pred)\n",
        "\n",
        "            results['Random Forest'] = {\n",
        "                'auc': rf_auc,\n",
        "                'ap': rf_ap,\n",
        "                'time': rf_time,\n",
        "                'predictions': rf_pred\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"    Random Forest failed: {e}\")\n",
        "            results['Random Forest'] = {'auc': 0.5, 'ap': 0.0, 'time': 0.0}\n",
        "\n",
        "        # Linear Regression\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            lr_pred = ml_baselines.linear_regression_predictor(\n",
        "                train_pairs, all_val_pairs, train_labels, true_labels\n",
        "            )\n",
        "            lr_time = time.time() - start_time\n",
        "\n",
        "            lr_auc = roc_auc_score(true_labels, lr_pred)\n",
        "            lr_ap = average_precision_score(true_labels, lr_pred)\n",
        "\n",
        "            results['Linear Regression'] = {\n",
        "                'auc': lr_auc,\n",
        "                'ap': lr_ap,\n",
        "                'time': lr_time,\n",
        "                'predictions': lr_pred\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"    Linear Regression failed: {e}\")\n",
        "            results['Linear Regression'] = {'auc': 0.5, 'ap': 0.0, 'time': 0.0}\n",
        "\n",
        "        # 4. Genetic Algorithm (reduced generations for speed)\n",
        "        print(\"  Evaluating Genetic Algorithm...\")\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            ga_optimizer = GeneticAlgorithmOptimizer(\n",
        "                data, population_size=20, generations=30\n",
        "            )\n",
        "            ga_pred = ga_optimizer.optimize(all_val_pairs)\n",
        "            ga_time = time.time() - start_time\n",
        "\n",
        "            if len(ga_pred) > 0 and np.max(ga_pred) > 0:\n",
        "                ga_pred = ga_pred / np.max(ga_pred)\n",
        "\n",
        "            ga_auc = roc_auc_score(true_labels, ga_pred) if len(np.unique(ga_pred)) > 1 else 0.5\n",
        "            ga_ap = average_precision_score(true_labels, ga_pred)\n",
        "\n",
        "            results['Genetic Algorithm'] = {\n",
        "                'auc': ga_auc,\n",
        "                'ap': ga_ap,\n",
        "                'time': ga_time,\n",
        "                'predictions': ga_pred\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"    Genetic Algorithm failed: {e}\")\n",
        "            results['Genetic Algorithm'] = {'auc': 0.5, 'ap': 0.0, 'time': 0.0}\n",
        "\n",
        "        # Print fold results\n",
        "        print(f\"\\n  Fold {fold + 1} Results:\")\n",
        "        for method, metrics in results.items():\n",
        "            print(f\"    {method}: AUC = {metrics['auc']:.4f}, AP = {metrics['ap']:.4f}\")\n",
        "\n",
        "        all_results.append(results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def visualize_comparative_results(results, network_data, save_path='qkd_comparative_results'):\n",
        "    \"\"\"Create comprehensive comparative visualizations\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    save_path = f\"{save_path}_{timestamp}\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Extract method names\n",
        "    methods = list(results[0].keys())\n",
        "\n",
        "    # Aggregate results across folds\n",
        "    aggregated_results = {}\n",
        "    for method in methods:\n",
        "        auc_scores = [fold[method]['auc'] for fold in results if method in fold]\n",
        "        ap_scores = [fold[method]['ap'] for fold in results if method in fold]\n",
        "\n",
        "        aggregated_results[method] = {\n",
        "            'auc_mean': np.mean(auc_scores),\n",
        "            'auc_std': np.std(auc_scores),\n",
        "            'ap_mean': np.mean(ap_scores),\n",
        "            'ap_std': np.std(ap_scores),\n",
        "            'auc_scores': auc_scores,\n",
        "            'ap_scores': ap_scores\n",
        "        }\n",
        "\n",
        "    # Create comparative plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # AUC Comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    method_names = list(aggregated_results.keys())\n",
        "    auc_means = [aggregated_results[m]['auc_mean'] for m in method_names]\n",
        "    auc_stds = [aggregated_results[m]['auc_std'] for m in method_names]\n",
        "\n",
        "    bars1 = ax1.bar(method_names, auc_means, yerr=auc_stds, capsize=5, alpha=0.7)\n",
        "    ax1.set_title('AUC Score Comparison')\n",
        "    ax1.set_ylabel('AUC Score')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Highlight best method\n",
        "    best_auc_idx = np.argmax(auc_means)\n",
        "    bars1[best_auc_idx].set_color('red')\n",
        "    bars1[best_auc_idx].set_alpha(1.0)\n",
        "\n",
        "    # AP Comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    ap_means = [aggregated_results[m]['ap_mean'] for m in method_names]\n",
        "    ap_stds = [aggregated_results[m]['ap_std'] for m in method_names]\n",
        "\n",
        "    bars2 = ax2.bar(method_names, ap_means, yerr=ap_stds, capsize=5, alpha=0.7)\n",
        "    ax2.set_title('Average Precision Comparison')\n",
        "    ax2.set_ylabel('Average Precision')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Highlight best method\n",
        "    best_ap_idx = np.argmax(ap_means)\n",
        "    bars2[best_ap_idx].set_color('red')\n",
        "    bars2[best_ap_idx].set_alpha(1.0)\n",
        "\n",
        "    # Box plots for score distributions\n",
        "    ax3 = axes[1, 0]\n",
        "    auc_data = [aggregated_results[m]['auc_scores'] for m in method_names]\n",
        "    box1 = ax3.boxplot(auc_data, labels=method_names, patch_artist=True)\n",
        "    ax3.set_title('AUC Score Distribution')\n",
        "    ax3.set_ylabel('AUC Score')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    ax4 = axes[1, 1]\n",
        "    ap_data = [aggregated_results[m]['ap_scores'] for m in method_names]\n",
        "    box2 = ax4.boxplot(ap_data, labels=method_names, patch_artist=True)\n",
        "    ax4.set_title('Average Precision Distribution')\n",
        "    ax4.set_ylabel('Average Precision')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/method_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Performance improvement analysis\n",
        "    gnn_auc = aggregated_results['GNN']['auc_mean']\n",
        "    gnn_ap = aggregated_results['GNN']['ap_mean']\n",
        "\n",
        "    improvements = {}\n",
        "    for method in method_names:\n",
        "        if method != 'GNN':\n",
        "            auc_improvement = ((gnn_auc - aggregated_results[method]['auc_mean']) /\n",
        "                             aggregated_results[method]['auc_mean']) * 100\n",
        "            ap_improvement = ((gnn_ap - aggregated_results[method]['ap_mean']) /\n",
        "                            aggregated_results[method]['ap_mean']) * 100\n",
        "\n",
        "            improvements[method] = {\n",
        "                'auc_improvement': auc_improvement,\n",
        "                'ap_improvement': ap_improvement\n",
        "            }\n",
        "\n",
        "    # Statistical significance testing\n",
        "    from scipy import stats\n",
        "\n",
        "    statistical_tests = {}\n",
        "    gnn_auc_scores = aggregated_results['GNN']['auc_scores']\n",
        "    gnn_ap_scores = aggregated_results['GNN']['ap_scores']\n",
        "\n",
        "    for method in method_names:\n",
        "        if method != 'GNN':\n",
        "            method_auc_scores = aggregated_results[method]['auc_scores']\n",
        "            method_ap_scores = aggregated_results[method]['ap_scores']\n",
        "\n",
        "            # Perform t-tests\n",
        "            auc_tstat, auc_pvalue = stats.ttest_rel(gnn_auc_scores, method_auc_scores)\n",
        "            ap_tstat, ap_pvalue = stats.ttest_rel(gnn_ap_scores, method_ap_scores)\n",
        "\n",
        "            statistical_tests[method] = {\n",
        "                'auc_tstat': float(auc_tstat),\n",
        "                'auc_pvalue': float(auc_pvalue),\n",
        "                'ap_tstat': float(ap_tstat),\n",
        "                'ap_pvalue': float(ap_pvalue),\n",
        "                'auc_significant': bool(auc_pvalue < 0.05),  # Convert to native bool\n",
        "                'ap_significant': bool(ap_pvalue < 0.05)     # Convert to native bool\n",
        "            }\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    report = {\n",
        "        'summary': {\n",
        "            'best_method_auc': method_names[best_auc_idx],\n",
        "            'best_auc_score': float(auc_means[best_auc_idx]),\n",
        "            'best_method_ap': method_names[best_ap_idx],\n",
        "            'best_ap_score': float(ap_means[best_ap_idx]),\n",
        "            'gnn_rank_auc': int(sorted(auc_means, reverse=True).index(gnn_auc) + 1),\n",
        "            'gnn_rank_ap': int(sorted(ap_means, reverse=True).index(gnn_ap) + 1)\n",
        "        },\n",
        "        'detailed_results': {},\n",
        "        'improvements_over_classical': improvements,\n",
        "        'statistical_significance': statistical_tests,\n",
        "        'network_characteristics': {\n",
        "            'num_nodes': int(network_data.num_nodes),\n",
        "            'num_edges': int(network_data.edge_index.size(1) // 2),\n",
        "            'avg_degree': float(network_data.edge_index.size(1) / network_data.num_nodes),\n",
        "            'avg_key_rate': float(network_data.edge_attr[:, 0].mean()),\n",
        "            'avg_qber': float(network_data.edge_attr[:, 1].mean())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add detailed results for each method\n",
        "    for method in method_names:\n",
        "        report['detailed_results'][method] = {\n",
        "            'auc_mean': float(aggregated_results[method]['auc_mean']),\n",
        "            'auc_std': float(aggregated_results[method]['auc_std']),\n",
        "            'ap_mean': float(aggregated_results[method]['ap_mean']),\n",
        "            'ap_std': float(aggregated_results[method]['ap_std'])\n",
        "        }\n",
        "\n",
        "    # Save report\n",
        "    with open(f\"{save_path}/comparative_analysis_report.json\", 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"COMPARATIVE ANALYSIS SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Best AUC: {report['summary']['best_method_auc']} ({report['summary']['best_auc_score']:.4f})\")\n",
        "    print(f\"Best AP:  {report['summary']['best_method_ap']} ({report['summary']['best_ap_score']:.4f})\")\n",
        "    print(f\"GNN Rank (AUC): {report['summary']['gnn_rank_auc']}/{len(method_names)}\")\n",
        "    print(f\"GNN Rank (AP):  {report['summary']['gnn_rank_ap']}/{len(method_names)}\")\n",
        "\n",
        "    print(f\"\\nGNN Improvements over Classical Methods:\")\n",
        "    for method, improvement in improvements.items():\n",
        "        print(f\"  vs {method}:\")\n",
        "        print(f\"    AUC: {improvement['auc_improvement']:+.2f}%\")\n",
        "        print(f\"    AP:  {improvement['ap_improvement']:+.2f}%\")\n",
        "\n",
        "    print(f\"\\nStatistical Significance (p < 0.05):\")\n",
        "    for method, test in statistical_tests.items():\n",
        "        print(f\"  vs {method}:\")\n",
        "        print(f\"    AUC: {'Significant' if test['auc_significant'] else 'Not significant'} (p={test['auc_pvalue']:.4f})\")\n",
        "        print(f\"    AP:  {'Significant' if test['ap_significant'] else 'Not significant'} (p={test['ap_pvalue']:.4f})\")\n",
        "\n",
        "    return report\n",
        "\n",
        "# Update the main execution to use comprehensive evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate network\n",
        "    print(\"Generating QKD network...\")\n",
        "    network = AdvancedQKDNetwork(num_nodes=30)  # Reduced for faster comparison\n",
        "    data = network.generate_graph_data()\n",
        "\n",
        "    # Create model\n",
        "    print(\"Creating GNN model...\")\n",
        "    model = AdvancedQKDLinkPredictor(\n",
        "        in_channels=data.x.size(1),\n",
        "        edge_attr_channels=data.edge_attr.size(1)\n",
        "    )\n",
        "\n",
        "    # Run comprehensive evaluation\n",
        "    print(\"Running comprehensive evaluation...\")\n",
        "    comparative_results = comprehensive_evaluation(model, data, num_epochs=50, k_folds=3)\n",
        "\n",
        "    # Generate comparative analysis\n",
        "    print(\"Generating comparative analysis...\")\n",
        "    analysis_report = visualize_comparative_results(comparative_results, data)\n",
        "\n",
        "    print(f\"\\nComparative analysis complete!\")\n",
        "    print(f\"Results demonstrate the relative performance of GNN vs classical methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# qkd_cross_topology_ablation.py\n",
        "# ------------------------------------------------\n",
        "# Cross-topology & Ablation study for your QKD GNN\n",
        "# Uses your AdvancedQuantumChannelSimulator and GNN model style,\n",
        "# extends AdvancedQKDNetwork to support ER/BA/WS/GEO topologies,\n",
        "# runs cross-topology tests and ablation experiments, prints + saves results.\n",
        "# ------------------------------------------------\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_undirected, negative_sampling\n",
        "from torch_geometric.nn import GATv2Conv, TransformerConv\n",
        "import networkx as nx\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------\n",
        "# Quantum channel simulator (your existing logic)\n",
        "# ---------------------------\n",
        "from scipy.constants import h, c\n",
        "\n",
        "class AdvancedQuantumChannelSimulator:\n",
        "    def __init__(self, distance, wavelength=1550e-9, fiber_loss=0.2,\n",
        "                 detector_efficiency=0.1, dark_count_rate=1e-6,\n",
        "                 atmospheric_visibility=None):\n",
        "        self.distance = float(distance)\n",
        "        self.wavelength = wavelength\n",
        "        self.fiber_loss = fiber_loss\n",
        "        self.detector_efficiency = detector_efficiency\n",
        "        self.dark_count_rate = dark_count_rate\n",
        "        self.atmospheric_visibility = atmospheric_visibility\n",
        "        self.photon_energy = h * c / wavelength\n",
        "\n",
        "    def calculate_channel_loss(self):\n",
        "        fiber_loss_db = self.fiber_loss * self.distance\n",
        "        fiber_transmission = 10 ** (-fiber_loss_db / 10.0)\n",
        "        if self.atmospheric_visibility:\n",
        "            beam_divergence = 1.22 * self.wavelength / 0.1\n",
        "            geometric_loss = (0.1 / (beam_divergence * max(self.distance, 1e-6))) ** 2\n",
        "            atmospheric_loss = np.exp(-3.91 * self.distance / self.atmospheric_visibility)\n",
        "            total_transmission = fiber_transmission * geometric_loss * atmospheric_loss\n",
        "        else:\n",
        "            total_transmission = fiber_transmission\n",
        "        return max(total_transmission, 1e-12)\n",
        "\n",
        "    def simulate_bb84_protocol(self, num_pulses=10000, mean_photon_number=0.1):\n",
        "        channel_transmission = self.calculate_channel_loss()\n",
        "        received_photons = np.random.poisson(\n",
        "            mean_photon_number * channel_transmission * self.detector_efficiency,\n",
        "            num_pulses\n",
        "        )\n",
        "        dark_counts = np.random.poisson(self.dark_count_rate, num_pulses)\n",
        "        total_counts = received_photons + dark_counts\n",
        "        basis_matches = np.random.choice([0, 1], num_pulses, p=[0.5, 0.5])\n",
        "        qber = min(0.5 * (1 - np.exp(-2 * self.distance / 100.0)), 0.49)\n",
        "        matched_pulses = total_counts * basis_matches\n",
        "        raw_key_rate = float(np.sum(matched_pulses)) / float(num_pulses)\n",
        "        final_key_rate = max(raw_key_rate * (1 - 2 * h2(qber)), 0.0)\n",
        "\n",
        "        return {\n",
        "            'qber': float(qber),\n",
        "            'raw_key_rate': float(raw_key_rate),\n",
        "            'final_key_rate': float(final_key_rate),\n",
        "            'channel_loss_db': float(-10.0 * np.log10(channel_transmission)),\n",
        "            'dark_count_probability': float(np.mean(dark_counts > 0))\n",
        "        }\n",
        "\n",
        "def h2(x):\n",
        "    if x <= 0.0 or x >= 1.0:\n",
        "        return 0.0\n",
        "    return -x * math.log2(x) - (1 - x) * math.log2(1 - x)\n",
        "\n",
        "# ---------------------------\n",
        "# Extended AdvancedQKDNetwork supporting multiple topologies\n",
        "# ---------------------------\n",
        "class AdvancedQKDNetwork:\n",
        "    \"\"\"\n",
        "    Generates PyG Data for a given topology type:\n",
        "      - 'GEO' : current spatial clustered placement (distance threshold)\n",
        "      - 'ER'  : Erdős–Rényi random graph\n",
        "      - 'BA'  : Barabási–Albert scale-free graph\n",
        "      - 'WS'  : Watts-Strogatz small-world graph\n",
        "    The QKD channel simulator is used to produce edge attributes (final_key_rate, qber, distance, loss, dark_prob).\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        config: dict with keys:\n",
        "          - num_nodes\n",
        "          - topology: 'GEO'|'ER'|'BA'|'WS'\n",
        "          - distance_threshold (for GEO) or used as radius when needed\n",
        "          - er_p, ba_m, ws_k, ws_p\n",
        "          - simulation params forwarded to AdvancedQuantumChannelSimulator\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.num_nodes = int(config.get('num_nodes', 50))\n",
        "        self.topology = config.get('topology', 'GEO').upper()\n",
        "        self.distance_threshold = config.get('distance_threshold', 100.0)\n",
        "        self.positions = self._generate_positions()\n",
        "\n",
        "    def _generate_positions(self):\n",
        "        # For GEO we use clustered Gaussian centers (your original)\n",
        "        if self.topology == 'GEO':\n",
        "            centers = np.random.multivariate_normal(\n",
        "                mean=[0, 0],\n",
        "                cov=[[self.config.get('center_cov', 100), 0], [0, self.config.get('center_cov', 100)]],\n",
        "                size=3\n",
        "            )\n",
        "            positions = []\n",
        "            for _ in range(self.num_nodes):\n",
        "                c = centers[np.random.randint(0, 3)]\n",
        "                pos = c + np.random.multivariate_normal(mean=[0, 0],\n",
        "                                                        cov=[[self.config.get('node_cov', 10), 0], [0, self.config.get('node_cov', 10)]])\n",
        "                positions.append(pos)\n",
        "            return np.array(positions, dtype=np.float32)\n",
        "        else:\n",
        "            # For topologies ER/BA/WS, sample positions uniformly (used to compute distances for edge attr)\n",
        "            pos = np.random.rand(self.num_nodes, 2).astype(np.float32) * 100.0\n",
        "            return pos\n",
        "\n",
        "    def generate_graph_data(self):\n",
        "        topology = self.topology\n",
        "        num_nodes = self.num_nodes\n",
        "\n",
        "        # Build graph structure using NetworkX\n",
        "        if topology == 'ER':\n",
        "            p = float(self.config.get('er_p', 0.08))\n",
        "            G = nx.erdos_renyi_graph(num_nodes, p)\n",
        "        elif topology == 'BA':\n",
        "            m = int(self.config.get('ba_m', 2))\n",
        "            # BA requires m < n\n",
        "            m = max(1, min(m, max(1, num_nodes - 1)))\n",
        "            G = nx.barabasi_albert_graph(num_nodes, m)\n",
        "        elif topology == 'WS':\n",
        "            k = int(self.config.get('ws_k', 4))\n",
        "            p = float(self.config.get('ws_p', 0.1))\n",
        "            # ensure k < num_nodes\n",
        "            k = min(k, max(1, num_nodes - 1))\n",
        "            if k % 2 == 1 and num_nodes % 2 == 0:\n",
        "                k = k + 1 if k + 1 < num_nodes else k - 1\n",
        "            G = nx.watts_strogatz_graph(num_nodes, k, p)\n",
        "        elif topology == 'GEO':\n",
        "            # Create empty graph, then add edges based on distance threshold\n",
        "            G = nx.Graph()\n",
        "            G.add_nodes_from(range(num_nodes))\n",
        "            # we'll add edges below if distance < threshold\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown topology: {topology}\")\n",
        "\n",
        "        # For GEO, add edges by distance threshold\n",
        "        edges = []\n",
        "        edge_attrs = []\n",
        "        if topology == 'GEO':\n",
        "            for i in range(num_nodes):\n",
        "                for j in range(i + 1, num_nodes):\n",
        "                    pi = self.positions[i]\n",
        "                    pj = self.positions[j]\n",
        "                    d = float(np.linalg.norm(pi - pj))\n",
        "                    if d <= self.distance_threshold:\n",
        "                        # simulate link\n",
        "                        use_fso = (np.random.rand() < self.config.get('fso_prob', 0.2))\n",
        "                        vis = self.config.get('atmospheric_visibility') if use_fso else None\n",
        "                        sim = AdvancedQuantumChannelSimulator(\n",
        "                            distance=d,\n",
        "                            wavelength=self.config.get('wavelength', 1550e-9),\n",
        "                            fiber_loss=self.config.get('fiber_loss', 0.2),\n",
        "                            detector_efficiency=self.config.get('detector_efficiency', 0.1),\n",
        "                            dark_count_rate=self.config.get('dark_count_rate', 1e-6),\n",
        "                            atmospheric_visibility=vis\n",
        "                        ).simulate_bb84_protocol()\n",
        "                        if sim['final_key_rate'] > 0:\n",
        "                            edges.append([i, j])\n",
        "                            edge_attrs.append([sim['final_key_rate'], sim['qber'], d, sim['channel_loss_db'], sim['dark_count_probability']])\n",
        "            if not edges:\n",
        "                raise RuntimeError(\"GEO topology yielded no edges; try increasing distance_threshold or parameters.\")\n",
        "        else:\n",
        "            # For ER/BA/WS, use G edges and compute QKD attributes using positions\n",
        "            for u, v in G.edges():\n",
        "                pi = self.positions[u]\n",
        "                pj = self.positions[v]\n",
        "                d = float(np.linalg.norm(pi - pj))\n",
        "                use_fso = (np.random.rand() < self.config.get('fso_prob', 0.2))\n",
        "                vis = self.config.get('atmospheric_visibility') if use_fso else None\n",
        "                sim = AdvancedQuantumChannelSimulator(\n",
        "                    distance=d,\n",
        "                    wavelength=self.config.get('wavelength', 1550e-9),\n",
        "                    fiber_loss=self.config.get('fiber_loss', 0.2),\n",
        "                    detector_efficiency=self.config.get('detector_efficiency', 0.1),\n",
        "                    dark_count_rate=self.config.get('dark_count_rate', 1e-6),\n",
        "                    atmospheric_visibility=vis\n",
        "                ).simulate_bb84_protocol()\n",
        "                if sim['final_key_rate'] > 0:\n",
        "                    edges.append([u, v])\n",
        "                    edge_attrs.append([sim['final_key_rate'], sim['qber'], d, sim['channel_loss_db'], sim['dark_count_probability']])\n",
        "\n",
        "            if not edges:\n",
        "                raise RuntimeError(f\"{topology} topology produced no viable edges (try different parameters).\")\n",
        "\n",
        "        # Build PyG Data\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(np.array(edge_attrs, dtype=np.float32), dtype=torch.float)\n",
        "\n",
        "        # Build node features (x, y, degree, betweenness)\n",
        "        G_for_metrics = nx.Graph()\n",
        "        G_for_metrics.add_edges_from(edges)\n",
        "        deg = dict(G_for_metrics.degree())\n",
        "        if len(G_for_metrics.edges()) > 0:\n",
        "            btw = nx.betweenness_centrality(G_for_metrics, normalized=True)\n",
        "        else:\n",
        "            btw = {i: 0.0 for i in range(num_nodes)}\n",
        "\n",
        "        node_features = []\n",
        "        for i in range(num_nodes):\n",
        "            pos = self.positions[i] if i < len(self.positions) else np.array([0.0, 0.0], dtype=np.float32)\n",
        "            node_features.append([float(pos[0]), float(pos[1]), float(deg.get(i, 0)), float(btw.get(i, 0))])\n",
        "\n",
        "        x = torch.tensor(np.array(node_features, dtype=np.float32), dtype=torch.float)\n",
        "\n",
        "        # to_undirected to follow your previous code convention\n",
        "        data = Data(x=x, edge_index=to_undirected(edge_index), edge_attr=edge_attr, pos=torch.tensor(self.positions, dtype=torch.float))\n",
        "        return data\n",
        "\n",
        "# ---------------------------\n",
        "# GNN model (your existing AdvancedQKDLinkPredictor)\n",
        "# ---------------------------\n",
        "class AdvancedQKDLinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, edge_attr_channels, hidden_channels=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = TransformerConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(edge_attr_channels, hidden_channels),\n",
        "            torch.nn.LayerNorm(hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "        )\n",
        "\n",
        "        self.link_predictor = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * hidden_channels, hidden_channels),\n",
        "            torch.nn.LayerNorm(hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(hidden_channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        z = self.conv1(x, edge_index)\n",
        "        z = torch.relu(z)\n",
        "        z = self.conv2(z, edge_index)\n",
        "        e = self.edge_mlp(edge_attr)\n",
        "        return z, e\n",
        "\n",
        "    def decode(self, z, edge_features, edge_label_index):\n",
        "        src, dst = edge_label_index\n",
        "        if edge_features.size(0) != edge_label_index.size(1):\n",
        "            edge_features = edge_features.mean(dim=0, keepdim=True).repeat(edge_label_index.size(1), 1)\n",
        "        node_pair = torch.cat([z[src], z[dst], edge_features], dim=-1)\n",
        "        return self.link_predictor(node_pair).squeeze(-1)\n",
        "\n",
        "# ---------------------------\n",
        "# Training helper (train full model on a dataset and return trained model)\n",
        "# ---------------------------\n",
        "def bce_with_logits_loss(pos_out, neg_out):\n",
        "    pos_loss = F.binary_cross_entropy_with_logits(pos_out, torch.ones_like(pos_out))\n",
        "    neg_loss = F.binary_cross_entropy_with_logits(neg_out, torch.zeros_like(neg_out))\n",
        "    return pos_loss + neg_loss\n",
        "\n",
        "def train_model_full(model, data, num_epochs=100, lr=1e-3, weight_decay=1e-2, device=None, early_stop_patience=20, verbose=True):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Build unique edges (undirected list)\n",
        "    edge_index_np = data.edge_index.cpu().numpy()\n",
        "    unique_edges = []\n",
        "    seen = set()\n",
        "    for i in range(edge_index_np.shape[1]):\n",
        "        a, b = int(edge_index_np[0, i]), int(edge_index_np[1, i])\n",
        "        key = tuple(sorted((a, b)))\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique_edges.append((a, b))\n",
        "    if len(unique_edges) == 0:\n",
        "        raise RuntimeError(\"No positive edges available for training\")\n",
        "\n",
        "    train_edge_index = torch.tensor(np.array(unique_edges).T, dtype=torch.long).to(device)\n",
        "    # Need edge_attr corresponding to unique edges; data.edge_attr is for unique edges (in our generation)\n",
        "    # If edge_attr length equals num unique edges, assume aligned with unique order (our generator does that).\n",
        "    if data.edge_attr.size(0) == train_edge_index.size(1):\n",
        "        train_edge_attr = data.edge_attr.to(device)\n",
        "    else:\n",
        "        # Attempt to map by matching pairs; fallback: use mean attr\n",
        "        train_edge_attr = data.edge_attr.to(device)\n",
        "        if train_edge_attr.size(0) == 0:\n",
        "            raise RuntimeError(\"Edge attr empty\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best_loss = float('inf')\n",
        "    patience = 0\n",
        "    model.train()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[TRAIN] Starting training: epochs={num_epochs}, device={device}, pos_edges={train_edge_index.size(1)}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        z, e = model(data.x, train_edge_index, train_edge_attr)\n",
        "        pos_out = model.decode(z, e, train_edge_index)\n",
        "\n",
        "        neg_edge_index = negative_sampling(train_edge_index, num_nodes=data.num_nodes, num_neg_samples=train_edge_index.size(1)).to(device)\n",
        "        neg_out = model.decode(z, e, neg_edge_index)\n",
        "\n",
        "        loss = bce_with_logits_loss(pos_out, neg_out)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
        "            print(f\"[TRAIN] Epoch {epoch:03d} | Loss {loss.item():.6f}\")\n",
        "\n",
        "        if loss.item() < best_loss - 1e-8:\n",
        "            best_loss = float(loss.item())\n",
        "            patience = 0\n",
        "            # save state dict\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= early_stop_patience:\n",
        "                print(f\"[TRAIN] Early stopping at epoch {epoch} (patience {patience})\")\n",
        "                break\n",
        "\n",
        "    # load best state\n",
        "    try:\n",
        "        model.load_state_dict(best_state)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluate trained model on a data graph (compute AUC/AP)\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_model_on_data(model, data, device=None, verbose=True):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Build unique positive edges\n",
        "    edge_index_np = data.edge_index.cpu().numpy()\n",
        "    unique_edges = []\n",
        "    seen = set()\n",
        "    for i in range(edge_index_np.shape[1]):\n",
        "        a, b = int(edge_index_np[0, i]), int(edge_index_np[1, i])\n",
        "        key = tuple(sorted((a, b)))\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique_edges.append((a, b))\n",
        "    if len(unique_edges) == 0:\n",
        "        if verbose:\n",
        "            print(\"[EVAL] No positive edges in graph\")\n",
        "        return {'auc': None, 'ap': None}\n",
        "\n",
        "    pos_edge_index = torch.tensor(np.array(unique_edges).T, dtype=torch.long).to(device)\n",
        "    # Map edge_attr if sizes match, else use mean\n",
        "    if data.edge_attr.size(0) == pos_edge_index.size(1):\n",
        "        edge_attr = data.edge_attr.to(device)\n",
        "    else:\n",
        "        # fallback: use mean of available edge_attr\n",
        "        edge_attr = data.edge_attr.to(device)\n",
        "        if edge_attr.size(0) == 0:\n",
        "            # create dummy small attr\n",
        "            edge_attr = torch.zeros((pos_edge_index.size(1), 5), device=device)\n",
        "\n",
        "    z, e = model(data.x, pos_edge_index, edge_attr)\n",
        "    pos_out = model.decode(z, e, pos_edge_index)\n",
        "\n",
        "    neg_edge_index = negative_sampling(pos_edge_index, num_nodes=data.num_nodes, num_neg_samples=pos_edge_index.size(1)).to(device)\n",
        "    neg_out = model.decode(z, e, neg_edge_index)\n",
        "\n",
        "    y_true = np.concatenate([np.ones(pos_out.size(0)), np.zeros(neg_out.size(0))])\n",
        "    y_scores = np.concatenate([pos_out.cpu().numpy(), neg_out.cpu().numpy()])\n",
        "\n",
        "    try:\n",
        "        auc = float(roc_auc_score(y_true, y_scores))\n",
        "    except Exception:\n",
        "        auc = float('nan')\n",
        "    try:\n",
        "        ap = float(average_precision_score(y_true, y_scores))\n",
        "    except Exception:\n",
        "        ap = float('nan')\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[EVAL] AUC={auc:.4f}, AP={ap:.4f} | pos={pos_out.size(0)} neg={neg_out.size(0)}\")\n",
        "\n",
        "    return {'auc': auc, 'ap': ap}\n",
        "\n",
        "# ---------------------------\n",
        "# Plotting functions\n",
        "# ---------------------------\n",
        "def plot_cross_topology_heatmap(results, topologies, out_dir, timestamp, metric='auc'):\n",
        "    data = np.full((len(topologies), len(topologies)), np.nan)\n",
        "    for r in results:\n",
        "        i = topologies.index(r['train_topology'])\n",
        "        j = topologies.index(r['test_topology'])\n",
        "        data[i, j] = r[metric.lower()]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(data, annot=True, fmt=\".4f\", cmap=\"viridis\", xticklabels=topologies, yticklabels=topologies)\n",
        "    plt.title(f\"Cross-Topology {metric.upper()} Heatmap\")\n",
        "    plt.xlabel(\"Test Topology\")\n",
        "    plt.ylabel(\"Train Topology\")\n",
        "    plot_path = os.path.join(out_dir, f\"cross_topology_{metric.lower()}_heatmap_{timestamp}.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "    print(f\"[PLOT] Saved cross-topology {metric} heatmap to: {plot_path}\")\n",
        "\n",
        "def plot_ablation_bar(results, topologies, out_dir, timestamp):\n",
        "    left_outs = [r['left_out'] for r in results]\n",
        "    aucs = [r['auc'] for r in results]\n",
        "    aps = [r['ap'] for r in results]\n",
        "\n",
        "    x = np.arange(len(left_outs))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.bar(x - width/2, aucs, width, label='AUC')\n",
        "    ax.bar(x + width/2, aps, width, label='AP')\n",
        "\n",
        "    ax.set_ylabel('Scores')\n",
        "    ax.set_title('Ablation Study Results (Leave-One-Out Topology)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(left_outs)\n",
        "    ax.legend()\n",
        "\n",
        "    plot_path = os.path.join(out_dir, f\"ablation_bar_chart_{timestamp}.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "    print(f\"[PLOT] Saved ablation bar chart to: {plot_path}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Cross-topology & Ablation orchestrator\n",
        "# ---------------------------\n",
        "def run_cross_topology_and_ablation(config):\n",
        "    topologies = config['topologies']\n",
        "    num_nodes = config['num_nodes']\n",
        "    out_dir = config['output_dir']\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"[MAIN] Device: {device} | topologies: {topologies} | nodes: {num_nodes}\")\n",
        "\n",
        "    # store results\n",
        "    cross_results = []\n",
        "    ablation_results = []\n",
        "\n",
        "    # 1) Cross-topology: train on each topology separately, test on all\n",
        "    print(\"\\n[MAIN] Starting cross-topology experiments (train on one topology, test on all)\")\n",
        "    for train_topo in topologies:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[MAIN] TRAIN TOPOLOGY: {train_topo}\")\n",
        "        cfg_train = dict(config)\n",
        "        cfg_train['topology'] = train_topo\n",
        "        net_train = AdvancedQKDNetwork(cfg_train)\n",
        "        data_train = net_train.generate_graph_data()\n",
        "        print(f\"[MAIN] Generated train graph for {train_topo}: nodes={data_train.num_nodes} edges={data_train.edge_index.size(1)//2}\")\n",
        "\n",
        "        model = AdvancedQKDLinkPredictor(in_channels=data_train.x.size(1), edge_attr_channels=data_train.edge_attr.size(1), hidden_channels=config['hidden']).to(device)\n",
        "        model = train_model_full(model, data_train, num_epochs=config['epochs'], lr=config['lr'], weight_decay=config['weight_decay'], device=device, early_stop_patience=config['early_stop_patience'], verbose=True)\n",
        "\n",
        "        # Evaluate on every topology\n",
        "        for test_topo in topologies:\n",
        "            cfg_test = dict(config)\n",
        "            cfg_test['topology'] = test_topo\n",
        "            net_test = AdvancedQKDNetwork(cfg_test)\n",
        "            data_test = net_test.generate_graph_data()\n",
        "            print(f\"[MAIN] Evaluating model trained on {train_topo} -> test on {test_topo}\")\n",
        "            metrics = evaluate_model_on_data(model, data_test, device=device)\n",
        "            cross_results.append({\n",
        "                'train_topology': train_topo,\n",
        "                'test_topology': test_topo,\n",
        "                'auc': metrics['auc'],\n",
        "                'ap': metrics['ap']\n",
        "            })\n",
        "\n",
        "    # Save cross results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    cross_json = os.path.join(out_dir, f\"cross_topology_results_{timestamp}.json\")\n",
        "    with open(cross_json, 'w') as f:\n",
        "        json.dump(cross_results, f, indent=2)\n",
        "    print(f\"[MAIN] Saved cross-topology results to: {cross_json}\")\n",
        "\n",
        "    # 2) Ablation: train on all but one topology, test on the left-out one\n",
        "    print(\"\\n[MAIN] Starting ablation experiments (train on all but one topology)\")\n",
        "    for left_out in topologies:\n",
        "        train_set = [t for t in topologies if t != left_out]\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[MAIN] ABLATION: leave out {left_out} -> train on {train_set}\")\n",
        "\n",
        "        # Create merged train data by concatenating edge lists and node features where possible.\n",
        "        # For simplicity we will create one combined dataset by generating multiple graphs and merging edges into a single large graph.\n",
        "        # This follows the \"same-dataset\" requirement: we do NOT use external datasets.\n",
        "        combined_cfg = dict(config)\n",
        "        combined_cfg['topology'] = train_set[0]\n",
        "        # start with first\n",
        "        combined_net = AdvancedQKDNetwork(combined_cfg)\n",
        "        combined_data = combined_net.generate_graph_data()\n",
        "\n",
        "        # Merge subsequent training topologies into combined_data\n",
        "        for t in train_set[1:]:\n",
        "            cfg_t = dict(config); cfg_t['topology'] = t\n",
        "            net_t = AdvancedQKDNetwork(cfg_t)\n",
        "            d = net_t.generate_graph_data()\n",
        "\n",
        "            # Merge node lists by simply concatenating nodes (we'll create a new graph where nodes count adds up).\n",
        "            # To keep things simple and consistent with link prediction pipeline, we'll re-index nodes of d and append.\n",
        "            base_n = combined_data.num_nodes\n",
        "            # reindex edges\n",
        "            new_edges = d.edge_index + base_n\n",
        "            combined_edge_index = torch.cat([combined_data.edge_index, new_edges], dim=1)\n",
        "            combined_edge_attr = torch.cat([combined_data.edge_attr, d.edge_attr], dim=0)\n",
        "            combined_x = torch.cat([combined_data.x, d.x], dim=0)\n",
        "            combined_pos = torch.cat([combined_data.pos, d.pos], dim=0)\n",
        "\n",
        "            combined_data = Data(x=combined_x, edge_index=combined_edge_index, edge_attr=combined_edge_attr, pos=combined_pos)\n",
        "\n",
        "        print(f\"[MAIN] Combined training data: nodes={combined_data.num_nodes} edges={combined_data.edge_index.size(1)//2}\")\n",
        "\n",
        "        # train on combined data\n",
        "        model = AdvancedQKDLinkPredictor(in_channels=combined_data.x.size(1), edge_attr_channels=combined_data.edge_attr.size(1), hidden_channels=config['hidden']).to(device)\n",
        "        model = train_model_full(model, combined_data, num_epochs=config['epochs'], lr=config['lr'], weight_decay=config['weight_decay'], device=device, early_stop_patience=config['early_stop_patience'], verbose=True)\n",
        "\n",
        "        # test on left_out topology\n",
        "        cfg_test = dict(config); cfg_test['topology'] = left_out\n",
        "        net_test = AdvancedQKDNetwork(cfg_test)\n",
        "        data_test = net_test.generate_graph_data()\n",
        "        print(f\"[MAIN] Evaluating ablation model -> test on left_out {left_out}\")\n",
        "        metrics = evaluate_model_on_data(model, data_test, device=device)\n",
        "        ablation_results.append({\n",
        "            'left_out': left_out,\n",
        "            'auc': metrics['auc'],\n",
        "            'ap': metrics['ap']\n",
        "        })\n",
        "\n",
        "    # Save ablation results\n",
        "    ablation_json = os.path.join(out_dir, f\"ablation_results_{timestamp}.json\")\n",
        "    with open(ablation_json, 'w') as f:\n",
        "        json.dump(ablation_results, f, indent=2)\n",
        "    print(f\"[MAIN] Saved ablation results to: {ablation_json}\")\n",
        "\n",
        "    # Print short summary\n",
        "    print(\"\\n\" + \"#\" * 60)\n",
        "    print(\"CROSS-TOPOLOGY SUMMARY\")\n",
        "    for r in cross_results:\n",
        "        print(f\"Train={r['train_topology']:<5} -> Test={r['test_topology']:<5} | AUC={r['auc']:.4f} | AP={r['ap']:.4f}\")\n",
        "    print(\"\\nABLATION SUMMARY (left-out topology):\")\n",
        "    for r in ablation_results:\n",
        "        print(f\"Left-out={r['left_out']:<5} | AUC={r['auc']:.4f} | AP={r['ap']:.4f}\")\n",
        "    print(\"#\" * 60)\n",
        "\n",
        "    # Generate plots\n",
        "    plot_cross_topology_heatmap(cross_results, topologies, out_dir, timestamp, metric='AUC')\n",
        "    plot_cross_topology_heatmap(cross_results, topologies, out_dir, timestamp, metric='AP')\n",
        "    plot_ablation_bar(ablation_results, topologies, out_dir, timestamp)\n",
        "\n",
        "    return {'cross': cross_results, 'ablation': ablation_results}\n",
        "\n",
        "# ---------------------------\n",
        "# CLI / main\n",
        "# ---------------------------\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description=\"QKD Cross-topology + Ablation study\")\n",
        "    p.add_argument('--num_nodes', type=int, default=50)\n",
        "    p.add_argument('--topologies', nargs='+', default=['GEO','ER','BA','WS'])\n",
        "    p.add_argument('--distance_threshold', type=float, default=40.0, help=\"distance cutoff for GEO (same units as positions)\")\n",
        "    p.add_argument('--fso_prob', type=float, default=0.2)\n",
        "    p.add_argument('--epochs', type=int, default=80)\n",
        "    p.add_argument('--lr', type=float, default=1e-3)\n",
        "    p.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    p.add_argument('--hidden', type=int, default=64)\n",
        "    p.add_argument('--output_dir', type=str, default='cross_ablation_outputs')\n",
        "    p.add_argument('--seed', type=int, default=42)\n",
        "    p.add_argument('--early_stop_patience', type=int, default=20)\n",
        "    args, _ = p.parse_known_args()\n",
        "    return args\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    config = {\n",
        "        'num_nodes': args.num_nodes,\n",
        "        'topologies': [t.upper() for t in args.topologies],\n",
        "        'topology': None,  # filled per-case\n",
        "        'distance_threshold': args.distance_threshold,\n",
        "        'fso_prob': args.fso_prob,\n",
        "        'wavelength': 1550e-9,\n",
        "        'fiber_loss': 0.2,\n",
        "        'detector_efficiency': 0.1,\n",
        "        'dark_count_rate': 1e-6,\n",
        "        'atmospheric_visibility': 20000.0,\n",
        "        'center_cov': 100,\n",
        "        'node_cov': 10,\n",
        "        'er_p': 0.08,\n",
        "        'ba_m': 2,\n",
        "        'ws_k': 4,\n",
        "        'ws_p': 0.1,\n",
        "        'epochs': args.epochs,\n",
        "        'lr': args.lr,\n",
        "        'weight_decay': args.weight_decay,\n",
        "        'hidden': args.hidden,\n",
        "        'output_dir': args.output_dir,\n",
        "        'seed': args.seed,\n",
        "        'early_stop_patience': args.early_stop_patience\n",
        "    }\n",
        "\n",
        "    start = time.time()\n",
        "    results = run_cross_topology_and_ablation(config)\n",
        "    end = time.time()\n",
        "    print(f\"\\nAll experiments done in {end - start:.1f}s. Results saved to {config['output_dir']}\")"
      ],
      "metadata": {
        "id": "50N8xy4rIH0t",
        "outputId": "29244669-6f4c-42c9-b358-4531db7a1edf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MAIN] Device: cpu | topologies: ['GEO', 'ER', 'BA', 'WS'] | nodes: 50\n",
            "\n",
            "[MAIN] Starting cross-topology experiments (train on one topology, test on all)\n",
            "\n",
            "================================================================================\n",
            "[MAIN] TRAIN TOPOLOGY: GEO\n",
            "[MAIN] Generated train graph for GEO: nodes=50 edges=718\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=718\n",
            "[TRAIN] Epoch 000 | Loss 1.597093\n",
            "[TRAIN] Epoch 010 | Loss 1.199824\n",
            "[TRAIN] Epoch 020 | Loss 0.958010\n",
            "[TRAIN] Epoch 030 | Loss 0.842332\n",
            "[TRAIN] Epoch 040 | Loss 0.702553\n",
            "[TRAIN] Epoch 050 | Loss 0.678682\n",
            "[TRAIN] Epoch 060 | Loss 0.594643\n",
            "[TRAIN] Epoch 070 | Loss 0.538557\n",
            "[TRAIN] Epoch 079 | Loss 0.515214\n",
            "[MAIN] Evaluating model trained on GEO -> test on GEO\n",
            "[EVAL] AUC=0.7939, AP=0.7849 | pos=998 neg=998\n",
            "[MAIN] Evaluating model trained on GEO -> test on ER\n",
            "[EVAL] AUC=0.8125, AP=0.8042 | pos=4 neg=4\n",
            "[MAIN] Evaluating model trained on GEO -> test on BA\n",
            "[EVAL] AUC=0.6667, AP=0.6389 | pos=3 neg=3\n",
            "[MAIN] Evaluating model trained on GEO -> test on WS\n",
            "[EVAL] AUC=0.7778, AP=0.8056 | pos=3 neg=3\n",
            "\n",
            "================================================================================\n",
            "[MAIN] TRAIN TOPOLOGY: ER\n",
            "[MAIN] Generated train graph for ER: nodes=50 edges=4\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=4\n",
            "[TRAIN] Epoch 000 | Loss 1.454252\n",
            "[TRAIN] Epoch 010 | Loss 0.567990\n",
            "[TRAIN] Epoch 020 | Loss 0.284708\n",
            "[TRAIN] Epoch 030 | Loss 0.187871\n",
            "[TRAIN] Epoch 040 | Loss 0.153964\n",
            "[TRAIN] Epoch 050 | Loss 0.160575\n",
            "[TRAIN] Epoch 060 | Loss 0.103705\n",
            "[TRAIN] Epoch 070 | Loss 0.121773\n",
            "[TRAIN] Epoch 079 | Loss 0.068543\n",
            "[MAIN] Evaluating model trained on ER -> test on GEO\n",
            "[EVAL] AUC=0.5361, AP=0.4960 | pos=557 neg=557\n",
            "[MAIN] Evaluating model trained on ER -> test on ER\n",
            "[EVAL] AUC=0.9200, AP=0.9429 | pos=5 neg=5\n",
            "[MAIN] Evaluating model trained on ER -> test on BA\n",
            "[EVAL] AUC=0.8800, AP=0.9250 | pos=5 neg=5\n",
            "[MAIN] Evaluating model trained on ER -> test on WS\n",
            "[EVAL] AUC=0.7500, AP=0.8333 | pos=2 neg=2\n",
            "\n",
            "================================================================================\n",
            "[MAIN] TRAIN TOPOLOGY: BA\n",
            "[MAIN] Generated train graph for BA: nodes=50 edges=4\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=4\n",
            "[TRAIN] Epoch 000 | Loss 1.425224\n",
            "[TRAIN] Epoch 010 | Loss 1.287627\n",
            "[TRAIN] Epoch 020 | Loss 1.367727\n",
            "[TRAIN] Epoch 030 | Loss 1.397505\n",
            "[TRAIN] Epoch 040 | Loss 0.494450\n",
            "[TRAIN] Epoch 050 | Loss 0.363800\n",
            "[TRAIN] Epoch 060 | Loss 0.312171\n",
            "[TRAIN] Epoch 070 | Loss 0.338285\n",
            "[TRAIN] Epoch 079 | Loss 0.246592\n",
            "[MAIN] Evaluating model trained on BA -> test on GEO\n",
            "[EVAL] AUC=0.4928, AP=0.5738 | pos=463 neg=463\n",
            "[MAIN] Evaluating model trained on BA -> test on ER\n",
            "[EVAL] AUC=0.5556, AP=0.7000 | pos=3 neg=3\n",
            "[MAIN] Evaluating model trained on BA -> test on BA\n",
            "[EVAL] AUC=0.5000, AP=0.7500 | pos=2 neg=2\n",
            "[MAIN] Evaluating model trained on BA -> test on WS\n",
            "[EVAL] AUC=0.6667, AP=0.8333 | pos=3 neg=3\n",
            "\n",
            "================================================================================\n",
            "[MAIN] TRAIN TOPOLOGY: WS\n",
            "[MAIN] Generated train graph for WS: nodes=50 edges=5\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=5\n",
            "[TRAIN] Epoch 000 | Loss 1.569739\n",
            "[TRAIN] Epoch 010 | Loss 0.586061\n",
            "[TRAIN] Epoch 020 | Loss 0.225094\n",
            "[TRAIN] Epoch 030 | Loss 0.171939\n",
            "[TRAIN] Epoch 040 | Loss 0.757758\n",
            "[TRAIN] Epoch 050 | Loss 0.145313\n",
            "[TRAIN] Epoch 060 | Loss 0.119999\n",
            "[TRAIN] Epoch 070 | Loss 0.115996\n",
            "[TRAIN] Epoch 079 | Loss 0.085123\n",
            "[MAIN] Evaluating model trained on WS -> test on GEO\n",
            "[EVAL] AUC=0.4265, AP=0.4428 | pos=972 neg=972\n",
            "[MAIN] Evaluating model trained on WS -> test on ER\n",
            "[EVAL] AUC=1.0000, AP=1.0000 | pos=6 neg=6\n",
            "[MAIN] Evaluating model trained on WS -> test on BA\n",
            "[EVAL] AUC=1.0000, AP=1.0000 | pos=3 neg=3\n",
            "[MAIN] Evaluating model trained on WS -> test on WS\n",
            "[EVAL] AUC=1.0000, AP=1.0000 | pos=3 neg=3\n",
            "[MAIN] Saved cross-topology results to: cross_ablation_outputs/cross_topology_results_20250905_135907.json\n",
            "\n",
            "[MAIN] Starting ablation experiments (train on all but one topology)\n",
            "\n",
            "================================================================================\n",
            "[MAIN] ABLATION: leave out GEO -> train on ['ER', 'BA', 'WS']\n",
            "[MAIN] Combined training data: nodes=150 edges=10\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=10\n",
            "[TRAIN] Epoch 000 | Loss 1.424243\n",
            "[TRAIN] Epoch 010 | Loss 1.302821\n",
            "[TRAIN] Epoch 020 | Loss 0.981558\n",
            "[TRAIN] Epoch 030 | Loss 0.796938\n",
            "[TRAIN] Epoch 040 | Loss 0.712364\n",
            "[TRAIN] Epoch 050 | Loss 0.536249\n",
            "[TRAIN] Epoch 060 | Loss 0.401208\n",
            "[TRAIN] Epoch 070 | Loss 0.621608\n",
            "[TRAIN] Epoch 079 | Loss 0.259273\n",
            "[MAIN] Evaluating ablation model -> test on left_out GEO\n",
            "[EVAL] AUC=0.6903, AP=0.7189 | pos=420 neg=420\n",
            "\n",
            "================================================================================\n",
            "[MAIN] ABLATION: leave out ER -> train on ['GEO', 'BA', 'WS']\n",
            "[MAIN] Combined training data: nodes=150 edges=616\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=616\n",
            "[TRAIN] Epoch 000 | Loss 1.453036\n",
            "[TRAIN] Epoch 010 | Loss 0.459093\n",
            "[TRAIN] Epoch 020 | Loss 0.454582\n",
            "[TRAIN] Epoch 030 | Loss 0.436633\n",
            "[TRAIN] Epoch 040 | Loss 0.395501\n",
            "[TRAIN] Epoch 050 | Loss 0.385340\n",
            "[TRAIN] Epoch 060 | Loss 0.320908\n",
            "[TRAIN] Epoch 070 | Loss 0.348763\n",
            "[TRAIN] Epoch 079 | Loss 0.291213\n",
            "[MAIN] Evaluating ablation model -> test on left_out ER\n",
            "[EVAL] AUC=0.3125, AP=0.4512 | pos=4 neg=4\n",
            "\n",
            "================================================================================\n",
            "[MAIN] ABLATION: leave out BA -> train on ['GEO', 'ER', 'WS']\n",
            "[MAIN] Combined training data: nodes=150 edges=648\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=648\n",
            "[TRAIN] Epoch 000 | Loss 1.484852\n",
            "[TRAIN] Epoch 010 | Loss 0.553072\n",
            "[TRAIN] Epoch 020 | Loss 0.438949\n",
            "[TRAIN] Epoch 030 | Loss 0.437578\n",
            "[TRAIN] Epoch 040 | Loss 0.382241\n",
            "[TRAIN] Epoch 050 | Loss 0.351116\n",
            "[TRAIN] Epoch 060 | Loss 0.330160\n",
            "[TRAIN] Epoch 070 | Loss 0.302418\n",
            "[TRAIN] Epoch 079 | Loss 0.316853\n",
            "[MAIN] Evaluating ablation model -> test on left_out BA\n",
            "[EVAL] AUC=0.2222, AP=0.4667 | pos=3 neg=3\n",
            "\n",
            "================================================================================\n",
            "[MAIN] ABLATION: leave out WS -> train on ['GEO', 'ER', 'BA']\n",
            "[MAIN] Combined training data: nodes=150 edges=593\n",
            "[TRAIN] Starting training: epochs=80, device=cpu, pos_edges=593\n",
            "[TRAIN] Epoch 000 | Loss 1.664824\n",
            "[TRAIN] Epoch 010 | Loss 0.545620\n",
            "[TRAIN] Epoch 020 | Loss 0.375104\n",
            "[TRAIN] Epoch 030 | Loss 0.374777\n",
            "[TRAIN] Epoch 040 | Loss 0.335484\n",
            "[TRAIN] Epoch 050 | Loss 0.299248\n",
            "[TRAIN] Epoch 060 | Loss 0.253187\n",
            "[TRAIN] Epoch 070 | Loss 0.250076\n",
            "[TRAIN] Epoch 079 | Loss 0.267193\n",
            "[MAIN] Evaluating ablation model -> test on left_out WS\n",
            "[EVAL] AUC=0.6000, AP=0.7500 | pos=5 neg=5\n",
            "[MAIN] Saved ablation results to: cross_ablation_outputs/ablation_results_20250905_135907.json\n",
            "\n",
            "############################################################\n",
            "CROSS-TOPOLOGY SUMMARY\n",
            "Train=GEO   -> Test=GEO   | AUC=0.7939 | AP=0.7849\n",
            "Train=GEO   -> Test=ER    | AUC=0.8125 | AP=0.8042\n",
            "Train=GEO   -> Test=BA    | AUC=0.6667 | AP=0.6389\n",
            "Train=GEO   -> Test=WS    | AUC=0.7778 | AP=0.8056\n",
            "Train=ER    -> Test=GEO   | AUC=0.5361 | AP=0.4960\n",
            "Train=ER    -> Test=ER    | AUC=0.9200 | AP=0.9429\n",
            "Train=ER    -> Test=BA    | AUC=0.8800 | AP=0.9250\n",
            "Train=ER    -> Test=WS    | AUC=0.7500 | AP=0.8333\n",
            "Train=BA    -> Test=GEO   | AUC=0.4928 | AP=0.5738\n",
            "Train=BA    -> Test=ER    | AUC=0.5556 | AP=0.7000\n",
            "Train=BA    -> Test=BA    | AUC=0.5000 | AP=0.7500\n",
            "Train=BA    -> Test=WS    | AUC=0.6667 | AP=0.8333\n",
            "Train=WS    -> Test=GEO   | AUC=0.4265 | AP=0.4428\n",
            "Train=WS    -> Test=ER    | AUC=1.0000 | AP=1.0000\n",
            "Train=WS    -> Test=BA    | AUC=1.0000 | AP=1.0000\n",
            "Train=WS    -> Test=WS    | AUC=1.0000 | AP=1.0000\n",
            "\n",
            "ABLATION SUMMARY (left-out topology):\n",
            "Left-out=GEO   | AUC=0.6903 | AP=0.7189\n",
            "Left-out=ER    | AUC=0.3125 | AP=0.4512\n",
            "Left-out=BA    | AUC=0.2222 | AP=0.4667\n",
            "Left-out=WS    | AUC=0.6000 | AP=0.7500\n",
            "############################################################\n",
            "[PLOT] Saved cross-topology AUC heatmap to: cross_ablation_outputs/cross_topology_auc_heatmap_20250905_135907.png\n",
            "[PLOT] Saved cross-topology AP heatmap to: cross_ablation_outputs/cross_topology_ap_heatmap_20250905_135907.png\n",
            "[PLOT] Saved ablation bar chart to: cross_ablation_outputs/ablation_bar_chart_20250905_135907.png\n",
            "\n",
            "All experiments done in 14.7s. Results saved to cross_ablation_outputs\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}