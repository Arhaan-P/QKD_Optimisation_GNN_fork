{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-6rMLy1lnBCQ",
        "outputId": "1793929f-2efa-4928-db7a-e7d0bd21a1ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: qiskit in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (2.1.1)\n",
            "Requirement already satisfied: qiskit-aer in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (0.17.1)\n",
            "Requirement already satisfied: torch in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (2.7.1)\n",
            "Requirement already satisfied: torch_geometric in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (3.5)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (3.10.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit) (0.16.0)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit) (2.3.2)\n",
            "Requirement already satisfied: scipy>=1.5 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit) (1.16.1)\n",
            "Requirement already satisfied: dill>=0.3 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit) (0.4.0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit) (4.14.1)\n",
            "Requirement already satisfied: psutil>=5 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit-aer) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from qiskit-aer) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch) (2025.7.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from python-dateutil>=2.8.0->qiskit-aer) (1.17.0)\n",
            "Requirement already satisfied: pbr>=2.0.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from stevedore>=3.0.0->qiskit) (6.1.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from requests->torch_geometric) (2025.7.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\ameiy\\documents\\4_personal_projects\\qkd_optimisation_gnn\\venv\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install qiskit qiskit-aer torch torch_geometric networkx matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjvkAHvgnBnX",
        "outputId": "c9455c26-7bd8-42c7-88ef-3da8f063fe3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating QKD network...\n",
            "Creating and training model...\n",
            "Using device: cpu\n",
            "\n",
            "Fold 1/5\n",
            "Epoch 10: Train Loss = 1.2917, Val Loss = 1.3077, AUC = 0.6956, AP = 0.6516\n",
            "Epoch 20: Train Loss = 1.1165, Val Loss = 1.2284, AUC = 0.7280, AP = 0.6972\n",
            "Epoch 30: Train Loss = 0.9381, Val Loss = 1.1720, AUC = 0.7566, AP = 0.7112\n",
            "Epoch 40: Train Loss = 0.7759, Val Loss = 1.2591, AUC = 0.7346, AP = 0.7111\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 2/5\n",
            "Epoch 10: Train Loss = 0.8287, Val Loss = 1.0229, AUC = 0.8384, AP = 0.8269\n",
            "Epoch 20: Train Loss = 0.7362, Val Loss = 1.2334, AUC = 0.7516, AP = 0.7067\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 3/5\n",
            "Epoch 10: Train Loss = 0.7097, Val Loss = 1.0497, AUC = 0.8249, AP = 0.8077\n",
            "Epoch 20: Train Loss = 0.6588, Val Loss = 1.1829, AUC = 0.7889, AP = 0.7409\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 4/5\n",
            "Epoch 10: Train Loss = 0.6401, Val Loss = 0.9622, AUC = 0.8453, AP = 0.7903\n",
            "Epoch 20: Train Loss = 0.6096, Val Loss = 1.0067, AUC = 0.8319, AP = 0.7866\n",
            "Epoch 30: Train Loss = 0.6646, Val Loss = 1.0684, AUC = 0.8023, AP = 0.7116\n",
            "Early stopping triggered\n",
            "\n",
            "Fold 5/5\n",
            "Epoch 10: Train Loss = 0.7357, Val Loss = 0.9398, AUC = 0.8632, AP = 0.8448\n",
            "Epoch 20: Train Loss = 0.6958, Val Loss = 1.0310, AUC = 0.8341, AP = 0.8035\n",
            "Early stopping triggered\n",
            "Generating visualizations and analysis...\n",
            "\n",
            "Analysis complete. Results saved in: qkd_results\n",
            "\n",
            "Summary Statistics:\n",
            "Number of nodes: 50\n",
            "Number of edges: 718\n",
            "Average degree: 28.72\n",
            "Average key rate: 1.04e+03 bits/s\n",
            "Average QBER: 0.061\n",
            "Model AUC: 0.804 ± 0.053\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATv2Conv, TransformerConv\n",
        "from torch_geometric.utils import to_undirected, negative_sampling\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
        "from scipy.constants import h, c\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def bce_with_logits_loss(pos_out, neg_out):\n",
        "    \"\"\"Custom BCE loss for link prediction\"\"\"\n",
        "    pos_loss = F.binary_cross_entropy_with_logits(\n",
        "        pos_out, torch.ones_like(pos_out)\n",
        "    )\n",
        "    neg_loss = F.binary_cross_entropy_with_logits(\n",
        "        neg_out, torch.zeros_like(neg_out)\n",
        "    )\n",
        "    return pos_loss + neg_loss\n",
        "\n",
        "def to_networkx(data):\n",
        "    \"\"\"Convert PyG data to NetworkX graph\"\"\"\n",
        "    G = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        G.add_edge(edge_index[0, i], edge_index[1, i])\n",
        "    return G\n",
        "\n",
        "class AdvancedQuantumChannelSimulator:\n",
        "    def __init__(self, distance, wavelength=1550e-9, fiber_loss=0.2,\n",
        "                 detector_efficiency=0.1, dark_count_rate=1e-6,\n",
        "                 atmospheric_visibility=None):\n",
        "        self.distance = distance\n",
        "        self.wavelength = wavelength\n",
        "        self.fiber_loss = fiber_loss\n",
        "        self.detector_efficiency = detector_efficiency\n",
        "        self.dark_count_rate = dark_count_rate\n",
        "        self.atmospheric_visibility = atmospheric_visibility\n",
        "        self.photon_energy = h * c / wavelength\n",
        "\n",
        "    def calculate_channel_loss(self):\n",
        "        fiber_loss_db = self.fiber_loss * self.distance\n",
        "        fiber_transmission = 10 ** (-fiber_loss_db/10)\n",
        "\n",
        "        if self.atmospheric_visibility:\n",
        "            beam_divergence = 1.22 * self.wavelength / 0.1\n",
        "            geometric_loss = (0.1 / (beam_divergence * self.distance)) ** 2\n",
        "            atmospheric_loss = np.exp(-3.91 * self.distance / self.atmospheric_visibility)\n",
        "            total_transmission = fiber_transmission * geometric_loss * atmospheric_loss\n",
        "        else:\n",
        "            total_transmission = fiber_transmission\n",
        "\n",
        "        return total_transmission\n",
        "\n",
        "    def simulate_bb84_protocol(self, num_pulses=10000, mean_photon_number=0.1):\n",
        "        channel_transmission = self.calculate_channel_loss()\n",
        "        received_photons = np.random.poisson(\n",
        "            mean_photon_number * channel_transmission * self.detector_efficiency,\n",
        "            num_pulses\n",
        "        )\n",
        "        dark_counts = np.random.poisson(self.dark_count_rate, num_pulses)\n",
        "        total_counts = received_photons + dark_counts\n",
        "        basis_matches = np.random.choice([0, 1], num_pulses, p=[0.5, 0.5])\n",
        "        qber = 0.5 * (1 - np.exp(-2 * self.distance / 100))\n",
        "        errors = np.random.choice([0, 1], num_pulses, p=[1-qber, qber])\n",
        "        matched_pulses = total_counts * basis_matches\n",
        "        raw_key_rate = np.sum(matched_pulses) / num_pulses\n",
        "        final_key_rate = raw_key_rate * (1 - 2 * h2(qber))\n",
        "\n",
        "        return {\n",
        "            'qber': qber,\n",
        "            'raw_key_rate': raw_key_rate,\n",
        "            'final_key_rate': final_key_rate,\n",
        "            'channel_loss_db': -10 * np.log10(channel_transmission),\n",
        "            'dark_count_probability': np.mean(dark_counts > 0)\n",
        "        }\n",
        "\n",
        "def h2(x):\n",
        "    \"\"\"Binary entropy function\"\"\"\n",
        "    return -x * np.log2(x) - (1-x) * np.log2(1-x) if 0 < x < 1 else 0\n",
        "\n",
        "class AdvancedQKDNetwork:\n",
        "    def __init__(self, num_nodes=50):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.positions = self._generate_realistic_topology()\n",
        "\n",
        "    def _generate_realistic_topology(self):\n",
        "        centers = np.random.multivariate_normal(\n",
        "            mean=[0, 0],\n",
        "            cov=[[100, 0], [0, 100]],\n",
        "            size=3\n",
        "        )\n",
        "\n",
        "        positions = []\n",
        "        for _ in range(self.num_nodes):\n",
        "            center = centers[np.random.randint(0, 3)]\n",
        "            pos = center + np.random.multivariate_normal(\n",
        "                mean=[0, 0],\n",
        "                cov=[[10, 0], [0, 10]]\n",
        "            )\n",
        "            positions.append(pos)\n",
        "\n",
        "        return np.array(positions)\n",
        "\n",
        "    def generate_graph_data(self):\n",
        "        distances = np.zeros((self.num_nodes, self.num_nodes))\n",
        "        for i in range(self.num_nodes):\n",
        "            for j in range(i + 1, self.num_nodes):\n",
        "                distances[i, j] = distances[j, i] = np.linalg.norm(\n",
        "                    self.positions[i] - self.positions[j]\n",
        "                )\n",
        "\n",
        "        edges = []\n",
        "        edge_attrs = []\n",
        "\n",
        "        for i in range(self.num_nodes):\n",
        "            for j in range(i + 1, self.num_nodes):\n",
        "                if distances[i, j] < 100:\n",
        "                    simulator = AdvancedQuantumChannelSimulator(\n",
        "                        distance=distances[i, j],\n",
        "                        atmospheric_visibility=20000 if np.random.random() < 0.2 else None\n",
        "                    )\n",
        "                    results = simulator.simulate_bb84_protocol()\n",
        "\n",
        "                    if results['final_key_rate'] > 0:\n",
        "                        edges.append([i, j])\n",
        "                        edge_attrs.append([\n",
        "                            results['final_key_rate'],\n",
        "                            results['qber'],\n",
        "                            distances[i, j],\n",
        "                            results['channel_loss_db'],\n",
        "                            results['dark_count_probability']\n",
        "                        ])\n",
        "\n",
        "        edge_index = torch.tensor(edges).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
        "\n",
        "        G = nx.Graph()\n",
        "        G.add_edges_from(edges)\n",
        "\n",
        "        node_features = []\n",
        "        for i in range(self.num_nodes):\n",
        "            features = [\n",
        "                self.positions[i, 0],\n",
        "                self.positions[i, 1],\n",
        "                G.degree(i) if i in G else 0,\n",
        "                nx.betweenness_centrality(G).get(i, 0) if i in G else 0\n",
        "            ]\n",
        "            node_features.append(features)\n",
        "\n",
        "        return Data(\n",
        "            x=torch.tensor(node_features, dtype=torch.float),\n",
        "            edge_index=to_undirected(edge_index),\n",
        "            edge_attr=edge_attr,\n",
        "            pos=torch.tensor(self.positions, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "class AdvancedQKDLinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, edge_attr_channels, hidden_channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = TransformerConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(edge_attr_channels, hidden_channels),\n",
        "            torch.nn.LayerNorm(hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "        )\n",
        "\n",
        "        self.link_predictor = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * hidden_channels, hidden_channels),\n",
        "            torch.nn.LayerNorm(hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(hidden_channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        # Process edge features for all edges\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "\n",
        "        return x, edge_features\n",
        "\n",
        "    def decode(self, z, edge_features, edge_label_index):\n",
        "        src, dst = edge_label_index\n",
        "\n",
        "        # Handle negative sampling case\n",
        "        if edge_features.size(0) != edge_label_index.size(1):\n",
        "            # For negative samples, create dummy edge features\n",
        "            edge_features = edge_features.mean(dim=0, keepdim=True).repeat(edge_label_index.size(1), 1)\n",
        "\n",
        "        node_features = torch.cat([\n",
        "            z[src],\n",
        "            z[dst],\n",
        "            edge_features\n",
        "        ], dim=-1)\n",
        "        return self.link_predictor(node_features).squeeze(-1)\n",
        "\n",
        "def train_and_evaluate(model, data, num_epochs=200, k_folds=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    all_results = []\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    edge_attr = data.edge_attr.cpu().numpy()\n",
        "    unique_edges = set()\n",
        "    edge_to_idx = {}\n",
        "\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        edge = tuple(sorted([edge_index[0, i], edge_index[1, i]]))\n",
        "        if edge not in unique_edges:\n",
        "            unique_edges.add(edge)\n",
        "            edge_to_idx[edge] = len(edge_to_idx)\n",
        "\n",
        "    unique_edges = list(unique_edges)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(unique_edges)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "\n",
        "        train_edges = [unique_edges[i] for i in train_idx]\n",
        "        val_edges = [unique_edges[i] for i in val_idx]\n",
        "\n",
        "        # Convert to numpy arrays first\n",
        "        train_edge_index = np.array([[edge[0], edge[1]] for edge in train_edges]).T\n",
        "        train_edge_attr = np.array([edge_attr[edge_to_idx[edge]] for edge in train_edges])\n",
        "\n",
        "        val_edge_index = np.array([[edge[0], edge[1]] for edge in val_edges]).T\n",
        "        val_edge_attr = np.array([edge_attr[edge_to_idx[edge]] for edge in val_edges])\n",
        "\n",
        "        # Convert to tensors\n",
        "        train_edge_index = torch.from_numpy(train_edge_index).to(device)\n",
        "        train_edge_attr = torch.from_numpy(train_edge_attr).float().to(device)\n",
        "        val_edge_index = torch.from_numpy(val_edge_index).to(device)\n",
        "        val_edge_attr = torch.from_numpy(val_edge_attr).float().to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "        train_losses = []\n",
        "        val_metrics = {'auc': [], 'ap': [], 'loss': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            z, edge_features = model(data.x, train_edge_index, train_edge_attr)\n",
        "            pos_out = model.decode(z, edge_features, train_edge_index)\n",
        "\n",
        "            # Generate negative samples\n",
        "            neg_edge_index = negative_sampling(\n",
        "                train_edge_index,\n",
        "                num_nodes=data.num_nodes,\n",
        "                num_neg_samples=train_edge_index.size(1)\n",
        "            )\n",
        "\n",
        "            neg_out = model.decode(z, edge_features, neg_edge_index)\n",
        "            loss = bce_with_logits_loss(pos_out, neg_out)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                z, edge_features = model(data.x, val_edge_index, val_edge_attr)\n",
        "                pos_out = model.decode(z, edge_features, val_edge_index)\n",
        "\n",
        "                neg_edge_index = negative_sampling(\n",
        "                    val_edge_index,\n",
        "                    num_nodes=data.num_nodes,\n",
        "                    num_neg_samples=val_edge_index.size(1)\n",
        "                )\n",
        "\n",
        "                neg_out = model.decode(z, edge_features, neg_edge_index)\n",
        "                val_loss = bce_with_logits_loss(pos_out, neg_out)\n",
        "\n",
        "                # Compute metrics\n",
        "                pred = torch.cat([pos_out, neg_out]).cpu().numpy()\n",
        "                true = torch.cat([\n",
        "                    torch.ones(pos_out.size(0)),\n",
        "                    torch.zeros(neg_out.size(0))\n",
        "                ]).numpy()\n",
        "\n",
        "                auc = roc_auc_score(true, pred)\n",
        "                ap = average_precision_score(true, pred)\n",
        "\n",
        "                val_metrics['auc'].append(auc)\n",
        "                val_metrics['ap'].append(ap)\n",
        "                val_metrics['loss'].append(val_loss.item())\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f\"Epoch {epoch + 1}: Train Loss = {loss:.4f}, \"\n",
        "                          f\"Val Loss = {val_loss:.4f}, AUC = {auc:.4f}, AP = {ap:.4f}\")\n",
        "\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    early_stopping_counter = 0\n",
        "                else:\n",
        "                    early_stopping_counter += 1\n",
        "\n",
        "                if early_stopping_counter >= 20:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        fold_results = {\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_metrics': val_metrics,\n",
        "            'final_auc': auc,\n",
        "            'final_ap': ap\n",
        "        }\n",
        "        all_results.append(fold_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def visualize_results(results, network_data, save_path='qkd_results'):\n",
        "    \"\"\"Create comprehensive visualizations and analysis\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    save_path = f\"{save_path}_{timestamp}\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Find the minimum length across all result arrays\n",
        "    min_epochs = min(len(result['train_losses']) for result in results)\n",
        "\n",
        "    # Truncate all arrays to the minimum length\n",
        "    truncated_results = []\n",
        "    for result in results:\n",
        "        truncated_result = {\n",
        "            'train_losses': result['train_losses'][:min_epochs],\n",
        "            'val_metrics': {\n",
        "                'auc': result['val_metrics']['auc'][:min_epochs],\n",
        "                'ap': result['val_metrics']['ap'][:min_epochs],\n",
        "                'loss': result['val_metrics']['loss'][:min_epochs]\n",
        "            },\n",
        "            'final_auc': result['final_auc'],\n",
        "            'final_ap': result['final_ap']\n",
        "        }\n",
        "        truncated_results.append(truncated_result)\n",
        "\n",
        "    # Training Metrics\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot training losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    for result in truncated_results:\n",
        "        plt.plot(result['train_losses'], alpha=0.3)\n",
        "    mean_train_loss = np.mean([r['train_losses'] for r in truncated_results], axis=0)\n",
        "    plt.plot(mean_train_loss, 'r-', label='Mean')\n",
        "    plt.title('Training Loss Evolution')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot validation AUC\n",
        "    plt.subplot(2, 2, 2)\n",
        "    for result in truncated_results:\n",
        "        plt.plot(result['val_metrics']['auc'], alpha=0.3)\n",
        "    mean_val_auc = np.mean([r['val_metrics']['auc'] for r in truncated_results], axis=0)\n",
        "    plt.plot(mean_val_auc, 'r-', label='Mean')\n",
        "    plt.title('Validation AUC Evolution')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    # Key Rate vs Distance Analysis\n",
        "    distances = network_data.edge_attr[:, 2].cpu().numpy()\n",
        "    key_rates = network_data.edge_attr[:, 0].cpu().numpy()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.scatter(distances, key_rates, alpha=0.5)\n",
        "    plt.xlabel('Distance (km)')\n",
        "    plt.ylabel('Key Rate (bits/s)')\n",
        "    plt.yscale('log')\n",
        "    plt.title('Key Rate vs Distance')\n",
        "\n",
        "    x_fit = np.linspace(min(distances), max(distances), 100)\n",
        "    y_fit = np.exp(-0.2 * x_fit)\n",
        "    plt.plot(x_fit, y_fit * max(key_rates), 'r--', label='Theoretical')\n",
        "    plt.legend()\n",
        "\n",
        "    # QBER Distribution\n",
        "    plt.subplot(2, 2, 4)\n",
        "    qber_values = network_data.edge_attr[:, 1].cpu().numpy()\n",
        "    sns.histplot(qber_values, bins=20)\n",
        "    plt.xlabel('QBER')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('QBER Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/training_metrics.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Performance Report\n",
        "    report = {\n",
        "        'network_stats': {\n",
        "            'num_nodes': int(network_data.num_nodes),\n",
        "            'num_edges': int(len(key_rates)),\n",
        "            'avg_degree': float(2 * len(key_rates) / network_data.num_nodes),\n",
        "            'avg_key_rate': float(np.mean(key_rates)),\n",
        "            'avg_qber': float(np.mean(qber_values)),\n",
        "            'max_distance': float(np.max(distances))\n",
        "        },\n",
        "        'model_performance': {\n",
        "            'final_metrics': {\n",
        "                'auc_mean': float(np.mean([r['final_auc'] for r in results])),\n",
        "                'auc_std': float(np.std([r['final_auc'] for r in results])),\n",
        "                'ap_mean': float(np.mean([r['final_ap'] for r in results])),\n",
        "                'ap_std': float(np.std([r['final_ap'] for r in results]))\n",
        "            },\n",
        "            'convergence': {\n",
        "                'final_train_loss_mean': float(np.mean([r['train_losses'][-1] for r in truncated_results])),\n",
        "                'best_epoch_mean': float(np.mean([np.argmin(r['val_metrics']['loss']) for r in truncated_results]))\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f\"{save_path}/performance_report.json\", 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "\n",
        "    return report\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate network\n",
        "    print(\"Generating QKD network...\")\n",
        "    network = AdvancedQKDNetwork(num_nodes=50)\n",
        "    data = network.generate_graph_data()\n",
        "\n",
        "    # Create and train model\n",
        "    print(\"Creating and training model...\")\n",
        "    model = AdvancedQKDLinkPredictor(\n",
        "        in_channels=data.x.size(1),\n",
        "        edge_attr_channels=data.edge_attr.size(1)\n",
        "    )\n",
        "\n",
        "    # Train and evaluate\n",
        "    results = train_and_evaluate(model, data)\n",
        "\n",
        "    # Generate visualizations and analysis\n",
        "    print(\"Generating visualizations and analysis...\")\n",
        "    save_path = 'qkd_results'\n",
        "    performance_report = visualize_results(results, data, save_path)\n",
        "\n",
        "    print(f\"\\nAnalysis complete. Results saved in: {save_path}\")\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.edge_index.size(1) // 2}\")\n",
        "    print(f\"Average degree: {data.edge_index.size(1) / data.num_nodes:.2f}\")\n",
        "    print(f\"Average key rate: {performance_report['network_stats']['avg_key_rate']:.2e} bits/s\")\n",
        "    print(f\"Average QBER: {performance_report['network_stats']['avg_qber']:.3f}\")\n",
        "    print(f\"Model AUC: {performance_report['model_performance']['final_metrics']['auc_mean']:.3f} ± \"\n",
        "          f\"{performance_report['model_performance']['final_metrics']['auc_std']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating QKD network...\n",
            "Creating GNN model...\n",
            "Running comprehensive evaluation...\n",
            "Using device: cpu\n",
            "Running comprehensive evaluation with classical baselines...\n",
            "Found 199 unique edges\n",
            "\n",
            "Fold 1/3\n",
            "Training GNN model...\n",
            "  GNN Epoch 0: Loss = 1.5601\n",
            "  GNN Epoch 10: Loss = 1.1354\n",
            "  GNN Epoch 20: Loss = 0.8901\n",
            "  GNN Epoch 30: Loss = 0.7825\n",
            "  GNN Epoch 40: Loss = 0.6684\n",
            "Evaluating all methods...\n",
            "  Evaluating GNN...\n",
            "  Evaluating classical routing methods...\n",
            "  Evaluating ML baselines...\n",
            "  Evaluating Genetic Algorithm...\n",
            "\n",
            "  Fold 1 Results:\n",
            "    GNN: AUC = 0.8209, AP = 0.7885\n",
            "    Dijkstra: AUC = 0.3904, AP = 0.4237\n",
            "    MST: AUC = 0.4053, AP = 0.4337\n",
            "    Greedy: AUC = 0.5376, AP = 0.5426\n",
            "    Random Forest: AUC = 0.5000, AP = 0.5000\n",
            "    Linear Regression: AUC = 0.5000, AP = 0.5000\n",
            "    Genetic Algorithm: AUC = 0.4908, AP = 0.5310\n",
            "\n",
            "Fold 2/3\n",
            "Training GNN model...\n",
            "  GNN Epoch 0: Loss = 0.9506\n",
            "  GNN Epoch 10: Loss = 0.8179\n",
            "  GNN Epoch 20: Loss = 0.6839\n",
            "  GNN Epoch 30: Loss = 0.7092\n",
            "  GNN Epoch 40: Loss = 0.6224\n",
            "Evaluating all methods...\n",
            "  Evaluating GNN...\n",
            "  Evaluating classical routing methods...\n",
            "  Evaluating ML baselines...\n",
            "  Evaluating Genetic Algorithm...\n",
            "\n",
            "  Fold 2 Results:\n",
            "    GNN: AUC = 0.7241, AP = 0.7173\n",
            "    Dijkstra: AUC = 0.4712, AP = 0.4814\n",
            "    MST: AUC = 0.4700, AP = 0.4942\n",
            "    Greedy: AUC = 0.5219, AP = 0.5616\n",
            "    Random Forest: AUC = 0.5000, AP = 0.5000\n",
            "    Linear Regression: AUC = 0.5000, AP = 0.5000\n",
            "    Genetic Algorithm: AUC = 0.4550, AP = 0.4741\n",
            "\n",
            "Fold 3/3\n",
            "Training GNN model...\n",
            "  GNN Epoch 0: Loss = 1.1506\n",
            "  GNN Epoch 10: Loss = 0.7788\n",
            "  GNN Epoch 20: Loss = 0.6273\n",
            "  GNN Epoch 30: Loss = 0.6896\n",
            "  GNN Epoch 40: Loss = 0.6236\n",
            "Evaluating all methods...\n",
            "  Evaluating GNN...\n",
            "  Evaluating classical routing methods...\n",
            "  Evaluating ML baselines...\n",
            "  Evaluating Genetic Algorithm...\n",
            "\n",
            "  Fold 3 Results:\n",
            "    GNN: AUC = 0.8251, AP = 0.7919\n",
            "    Dijkstra: AUC = 0.3753, AP = 0.4251\n",
            "    MST: AUC = 0.4198, AP = 0.4651\n",
            "    Greedy: AUC = 0.5251, AP = 0.4925\n",
            "    Random Forest: AUC = 0.5000, AP = 0.5000\n",
            "    Linear Regression: AUC = 0.5000, AP = 0.5000\n",
            "    Genetic Algorithm: AUC = 0.5514, AP = 0.5263\n",
            "Generating comparative analysis...\n",
            "\n",
            "============================================================\n",
            "COMPARATIVE ANALYSIS SUMMARY\n",
            "============================================================\n",
            "Best AUC: GNN (0.7900)\n",
            "Best AP:  GNN (0.7659)\n",
            "GNN Rank (AUC): 1/7\n",
            "GNN Rank (AP):  1/7\n",
            "\n",
            "GNN Improvements over Classical Methods:\n",
            "  vs Dijkstra:\n",
            "    AUC: +91.60%\n",
            "    AP:  +72.73%\n",
            "  vs MST:\n",
            "    AUC: +82.99%\n",
            "    AP:  +64.94%\n",
            "  vs Greedy:\n",
            "    AUC: +49.56%\n",
            "    AP:  +43.91%\n",
            "  vs Random Forest:\n",
            "    AUC: +58.00%\n",
            "    AP:  +53.18%\n",
            "  vs Linear Regression:\n",
            "    AUC: +58.00%\n",
            "    AP:  +53.18%\n",
            "  vs Genetic Algorithm:\n",
            "    AUC: +58.30%\n",
            "    AP:  +50.05%\n",
            "\n",
            "Statistical Significance (p < 0.05):\n",
            "  vs Dijkstra:\n",
            "    AUC: Significant (p=0.0264)\n",
            "    AP:  Significant (p=0.0175)\n",
            "  vs MST:\n",
            "    AUC: Significant (p=0.0206)\n",
            "    AP:  Significant (p=0.0172)\n",
            "  vs Greedy:\n",
            "    AUC: Significant (p=0.0131)\n",
            "    AP:  Significant (p=0.0307)\n",
            "  vs Random Forest:\n",
            "    AUC: Significant (p=0.0127)\n",
            "    AP:  Significant (p=0.0083)\n",
            "  vs Linear Regression:\n",
            "    AUC: Significant (p=0.0127)\n",
            "    AP:  Significant (p=0.0083)\n",
            "  vs Genetic Algorithm:\n",
            "    AUC: Significant (p=0.0045)\n",
            "    AP:  Significant (p=0.0007)\n",
            "\n",
            "Comparative analysis complete!\n",
            "Results demonstrate the relative performance of GNN vs classical methods.\n"
          ]
        }
      ],
      "source": [
        "# Add these imports to the existing imports cell\n",
        "import heapq\n",
        "from scipy.optimize import differential_evolution\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ClassicalRoutingMethods:\n",
        "    \"\"\"Classical routing and optimization methods for QKD networks\"\"\"\n",
        "    \n",
        "    def __init__(self, network_data):\n",
        "        self.data = network_data\n",
        "        self.G = self._build_networkx_graph()\n",
        "        \n",
        "    def _build_networkx_graph(self):\n",
        "        \"\"\"Build NetworkX graph from PyTorch Geometric data\"\"\"\n",
        "        G = nx.Graph()\n",
        "        edge_index = self.data.edge_index.cpu().numpy()\n",
        "        edge_attr = self.data.edge_attr.cpu().numpy()\n",
        "        \n",
        "        # Handle the case where edges are duplicated in edge_index but not in edge_attr\n",
        "        # Create a mapping from edge to attributes\n",
        "        edge_to_attr = {}\n",
        "        \n",
        "        # Since we use to_undirected, edges are duplicated in edge_index\n",
        "        # But edge_attr contains only unique edges\n",
        "        num_unique_edges = edge_attr.shape[0]\n",
        "        \n",
        "        for i in range(num_unique_edges):\n",
        "            # For each unique edge, we need to find its position in the original edge_index\n",
        "            # Since to_undirected creates [src,dst] and [dst,src], we handle both\n",
        "            if i < edge_index.shape[1] // 2:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            else:\n",
        "                # For the second half, edges are reversed\n",
        "                original_idx = i - edge_index.shape[1] // 2\n",
        "                dst, src = edge_index[0, original_idx], edge_index[1, original_idx]\n",
        "            \n",
        "            edge_key = tuple(sorted([src, dst]))\n",
        "            if edge_key not in edge_to_attr:\n",
        "                key_rate = edge_attr[i % num_unique_edges, 0]\n",
        "                qber = edge_attr[i % num_unique_edges, 1]\n",
        "                distance = edge_attr[i % num_unique_edges, 2]\n",
        "                edge_to_attr[edge_key] = (key_rate, qber, distance)\n",
        "        \n",
        "        # Now build the graph with unique edges\n",
        "        for edge_key, (key_rate, qber, distance) in edge_to_attr.items():\n",
        "            src, dst = edge_key\n",
        "            \n",
        "            # Use negative key rate as weight for shortest path algorithms\n",
        "            weight = 1.0 / (key_rate + 1e-10)  # Avoid division by zero\n",
        "            \n",
        "            G.add_edge(src, dst, \n",
        "                      weight=weight,\n",
        "                      key_rate=key_rate,\n",
        "                      qber=qber,\n",
        "                      distance=distance)\n",
        "        \n",
        "        return G\n",
        "    \n",
        "    def dijkstra_routing(self, source_target_pairs):\n",
        "        \"\"\"Dijkstra's shortest path algorithm\"\"\"\n",
        "        predictions = []\n",
        "        \n",
        "        for src, dst in source_target_pairs:\n",
        "            try:\n",
        "                # Find shortest path based on inverse key rate\n",
        "                path = nx.shortest_path(self.G, src, dst, weight='weight')\n",
        "                path_exists = len(path) > 1\n",
        "                \n",
        "                if path_exists:\n",
        "                    # Calculate path quality metrics\n",
        "                    total_key_rate = 1.0\n",
        "                    max_qber = 0.0\n",
        "                    total_distance = 0.0\n",
        "                    \n",
        "                    for i in range(len(path) - 1):\n",
        "                        edge_data = self.G[path[i]][path[i+1]]\n",
        "                        total_key_rate *= edge_data['key_rate']\n",
        "                        max_qber = max(max_qber, edge_data['qber'])\n",
        "                        total_distance += edge_data['distance']\n",
        "                    \n",
        "                    # Score based on end-to-end key rate\n",
        "                    score = total_key_rate if max_qber < 0.11 else 0.0\n",
        "                else:\n",
        "                    score = 0.0\n",
        "                    \n",
        "            except nx.NetworkXNoPath:\n",
        "                score = 0.0\n",
        "            except KeyError:\n",
        "                score = 0.0\n",
        "                \n",
        "            predictions.append(score)\n",
        "            \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def minimum_spanning_tree(self, source_target_pairs):\n",
        "        \"\"\"MST-based routing\"\"\"\n",
        "        try:\n",
        "            mst = nx.minimum_spanning_tree(self.G, weight='weight')\n",
        "        except:\n",
        "            # If MST fails, use original graph\n",
        "            mst = self.G\n",
        "        \n",
        "        predictions = []\n",
        "        for src, dst in source_target_pairs:\n",
        "            try:\n",
        "                path = nx.shortest_path(mst, src, dst)\n",
        "                if len(path) > 1:\n",
        "                    # Calculate path quality\n",
        "                    total_key_rate = 1.0\n",
        "                    for i in range(len(path) - 1):\n",
        "                        if mst.has_edge(path[i], path[i+1]):\n",
        "                            edge_data = mst[path[i]][path[i+1]]\n",
        "                            total_key_rate *= edge_data.get('key_rate', 0.001)\n",
        "                    score = total_key_rate\n",
        "                else:\n",
        "                    score = 0.0\n",
        "            except (nx.NetworkXNoPath, KeyError):\n",
        "                score = 0.0\n",
        "            predictions.append(score)\n",
        "            \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def greedy_best_first(self, source_target_pairs):\n",
        "        \"\"\"Greedy best-first search based on key rate\"\"\"\n",
        "        predictions = []\n",
        "        \n",
        "        for src, dst in source_target_pairs:\n",
        "            try:\n",
        "                visited = set()\n",
        "                current = src\n",
        "                path_key_rate = 1.0\n",
        "                found_path = False\n",
        "                max_hops = 10  # Prevent infinite loops\n",
        "                \n",
        "                while current != dst and current not in visited and len(visited) < max_hops:\n",
        "                    visited.add(current)\n",
        "                    \n",
        "                    # Find best next hop based on key rate\n",
        "                    best_neighbor = None\n",
        "                    best_key_rate = 0.0\n",
        "                    \n",
        "                    if current in self.G:\n",
        "                        for neighbor in self.G.neighbors(current):\n",
        "                            if neighbor not in visited:\n",
        "                                edge_data = self.G[current][neighbor]\n",
        "                                if edge_data['key_rate'] > best_key_rate:\n",
        "                                    best_key_rate = edge_data['key_rate']\n",
        "                                    best_neighbor = neighbor\n",
        "                    \n",
        "                    if best_neighbor is None:\n",
        "                        break\n",
        "                        \n",
        "                    path_key_rate *= best_key_rate\n",
        "                    current = best_neighbor\n",
        "                    \n",
        "                    if current == dst:\n",
        "                        found_path = True\n",
        "                        break\n",
        "                \n",
        "                score = path_key_rate if found_path else 0.0\n",
        "            except:\n",
        "                score = 0.0\n",
        "                \n",
        "            predictions.append(score)\n",
        "            \n",
        "        return np.array(predictions)\n",
        "\n",
        "class GeneticAlgorithmOptimizer:\n",
        "    \"\"\"Genetic Algorithm for QKD network optimization\"\"\"\n",
        "    \n",
        "    def __init__(self, network_data, population_size=50, generations=100):\n",
        "        self.data = network_data\n",
        "        self.population_size = population_size\n",
        "        self.generations = generations\n",
        "        self.num_nodes = network_data.num_nodes\n",
        "        \n",
        "        # Build adjacency matrix for faster access\n",
        "        self.adj_matrix = self._build_adjacency_matrix()\n",
        "        \n",
        "    def _build_adjacency_matrix(self):\n",
        "        \"\"\"Build adjacency matrix with key rates\"\"\"\n",
        "        adj = np.zeros((self.num_nodes, self.num_nodes))\n",
        "        edge_index = self.data.edge_index.cpu().numpy()\n",
        "        edge_attr = self.data.edge_attr.cpu().numpy()\n",
        "        \n",
        "        # Handle undirected edges correctly\n",
        "        num_unique_edges = edge_attr.shape[0]\n",
        "        \n",
        "        for i in range(min(edge_index.shape[1], num_unique_edges)):\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            key_rate = edge_attr[i, 0]\n",
        "            adj[src, dst] = key_rate\n",
        "            adj[dst, src] = key_rate\n",
        "            \n",
        "        return adj\n",
        "    \n",
        "    def _fitness_function(self, individual, source_target_pairs):\n",
        "        \"\"\"Evaluate fitness of an individual (routing solution)\"\"\"\n",
        "        total_score = 0.0\n",
        "        \n",
        "        for i, (src, dst) in enumerate(source_target_pairs):\n",
        "            # Individual encodes path selection probabilities\n",
        "            path_score = self._evaluate_path(src, dst, individual)\n",
        "            total_score += path_score\n",
        "            \n",
        "        return total_score / max(len(source_target_pairs), 1)\n",
        "    \n",
        "    def _evaluate_path(self, src, dst, individual):\n",
        "        \"\"\"Evaluate a specific path using individual's strategy\"\"\"\n",
        "        current = src\n",
        "        visited = set()\n",
        "        path_key_rate = 1.0\n",
        "        \n",
        "        while current != dst and current not in visited and len(visited) < 10:\n",
        "            visited.add(current)\n",
        "            \n",
        "            # Use individual's weights to select next hop\n",
        "            neighbors = np.where(self.adj_matrix[current] > 0)[0]\n",
        "            neighbors = [n for n in neighbors if n not in visited]\n",
        "            \n",
        "            if not neighbors:\n",
        "                return 0.0\n",
        "                \n",
        "            # Weight neighbors based on individual's preferences\n",
        "            weights = []\n",
        "            for neighbor in neighbors:\n",
        "                key_rate = self.adj_matrix[current, neighbor]\n",
        "                # Individual influences neighbor selection\n",
        "                weight = key_rate * (1 + individual[neighbor % len(individual)])\n",
        "                weights.append(weight)\n",
        "            \n",
        "            if not weights:\n",
        "                return 0.0\n",
        "                \n",
        "            # Select best neighbor\n",
        "            best_idx = np.argmax(weights)\n",
        "            next_node = neighbors[best_idx]\n",
        "            path_key_rate *= self.adj_matrix[current, next_node]\n",
        "            current = next_node\n",
        "            \n",
        "        return path_key_rate if current == dst else 0.0\n",
        "    \n",
        "    def optimize(self, source_target_pairs):\n",
        "        \"\"\"Run genetic algorithm optimization\"\"\"\n",
        "        if not source_target_pairs:\n",
        "            return np.array([])\n",
        "            \n",
        "        # Initialize population\n",
        "        population = []\n",
        "        for _ in range(self.population_size):\n",
        "            individual = np.random.uniform(-1, 1, self.num_nodes)\n",
        "            population.append(individual)\n",
        "        \n",
        "        best_fitness = -float('inf')\n",
        "        best_individual = None\n",
        "        \n",
        "        for generation in range(self.generations):\n",
        "            # Evaluate fitness\n",
        "            fitness_scores = []\n",
        "            for individual in population:\n",
        "                fitness = self._fitness_function(individual, source_target_pairs)\n",
        "                fitness_scores.append(fitness)\n",
        "                \n",
        "                if fitness > best_fitness:\n",
        "                    best_fitness = fitness\n",
        "                    best_individual = individual.copy()\n",
        "            \n",
        "            # Selection and reproduction\n",
        "            new_population = []\n",
        "            \n",
        "            # Elitism: keep best individuals\n",
        "            elite_size = max(1, self.population_size // 10)\n",
        "            elite_indices = np.argsort(fitness_scores)[-elite_size:]\n",
        "            for idx in elite_indices:\n",
        "                new_population.append(population[idx].copy())\n",
        "            \n",
        "            # Crossover and mutation\n",
        "            while len(new_population) < self.population_size:\n",
        "                # Tournament selection\n",
        "                parent1 = self._tournament_selection(population, fitness_scores)\n",
        "                parent2 = self._tournament_selection(population, fitness_scores)\n",
        "                \n",
        "                # Crossover\n",
        "                child = self._crossover(parent1, parent2)\n",
        "                \n",
        "                # Mutation\n",
        "                child = self._mutate(child)\n",
        "                \n",
        "                new_population.append(child)\n",
        "            \n",
        "            population = new_population\n",
        "        \n",
        "        # Generate predictions using best individual\n",
        "        if best_individual is None:\n",
        "            return np.zeros(len(source_target_pairs))\n",
        "            \n",
        "        predictions = []\n",
        "        for src, dst in source_target_pairs:\n",
        "            score = self._evaluate_path(src, dst, best_individual)\n",
        "            predictions.append(score)\n",
        "            \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def _tournament_selection(self, population, fitness_scores, tournament_size=3):\n",
        "        \"\"\"Tournament selection for parent selection\"\"\"\n",
        "        tournament_size = min(tournament_size, len(population))\n",
        "        tournament_indices = np.random.choice(len(population), tournament_size, replace=False)\n",
        "        tournament_fitness = [fitness_scores[i] for i in tournament_indices]\n",
        "        winner_idx = tournament_indices[np.argmax(tournament_fitness)]\n",
        "        return population[winner_idx].copy()\n",
        "    \n",
        "    def _crossover(self, parent1, parent2):\n",
        "        \"\"\"Single-point crossover\"\"\"\n",
        "        crossover_point = np.random.randint(1, len(parent1))\n",
        "        child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "        return child\n",
        "    \n",
        "    def _mutate(self, individual, mutation_rate=0.1):\n",
        "        \"\"\"Gaussian mutation\"\"\"\n",
        "        mask = np.random.random(len(individual)) < mutation_rate\n",
        "        individual[mask] += np.random.normal(0, 0.1, np.sum(mask))\n",
        "        return np.clip(individual, -1, 1)\n",
        "\n",
        "class MachineLearningBaselines:\n",
        "    \"\"\"Classical ML approaches for link prediction\"\"\"\n",
        "    \n",
        "    def __init__(self, network_data):\n",
        "        self.data = network_data\n",
        "        \n",
        "    def random_forest_predictor(self, train_pairs, test_pairs, train_labels, test_labels):\n",
        "        \"\"\"Random Forest baseline\"\"\"\n",
        "        # Extract features for training\n",
        "        train_features = self._extract_features(train_pairs)\n",
        "        test_features = self._extract_features(test_pairs)\n",
        "        \n",
        "        if train_features.shape[0] == 0 or test_features.shape[0] == 0:\n",
        "            return np.zeros(len(test_pairs))\n",
        "        \n",
        "        # Train Random Forest\n",
        "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf.fit(train_features, train_labels)\n",
        "        \n",
        "        # Predict\n",
        "        predictions = rf.predict(test_features)\n",
        "        return predictions\n",
        "    \n",
        "    def linear_regression_predictor(self, train_pairs, test_pairs, train_labels, test_labels):\n",
        "        \"\"\"Linear Regression baseline\"\"\"\n",
        "        train_features = self._extract_features(train_pairs)\n",
        "        test_features = self._extract_features(test_pairs)\n",
        "        \n",
        "        if train_features.shape[0] == 0 or test_features.shape[0] == 0:\n",
        "            return np.zeros(len(test_pairs))\n",
        "        \n",
        "        lr = LinearRegression()\n",
        "        lr.fit(train_features, train_labels)\n",
        "        \n",
        "        predictions = lr.predict(test_features)\n",
        "        return predictions\n",
        "    \n",
        "    def _extract_features(self, node_pairs):\n",
        "        \"\"\"Extract features for node pairs\"\"\"\n",
        "        if not node_pairs:\n",
        "            return np.array([]).reshape(0, 10)  # Return empty array with correct shape\n",
        "            \n",
        "        features = []\n",
        "        node_features = self.data.x.cpu().numpy()\n",
        "        edge_index = self.data.edge_index.cpu().numpy()\n",
        "        \n",
        "        # Build adjacency list for faster lookup\n",
        "        adj_list = {}\n",
        "        num_unique_edges = min(edge_index.shape[1], self.data.edge_attr.shape[0])\n",
        "        \n",
        "        for i in range(num_unique_edges):\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            if src not in adj_list:\n",
        "                adj_list[src] = []\n",
        "            if dst not in adj_list:\n",
        "                adj_list[dst] = []\n",
        "            adj_list[src].append(dst)\n",
        "            adj_list[dst].append(src)\n",
        "        \n",
        "        for src, dst in node_pairs:\n",
        "            # Node features\n",
        "            src_features = node_features[src] if src < len(node_features) else np.zeros(4)\n",
        "            dst_features = node_features[dst] if dst < len(node_features) else np.zeros(4)\n",
        "            \n",
        "            # Distance between nodes\n",
        "            distance = np.linalg.norm(src_features[:2] - dst_features[:2])\n",
        "            \n",
        "            # Common neighbors\n",
        "            src_neighbors = set(adj_list.get(src, []))\n",
        "            dst_neighbors = set(adj_list.get(dst, []))\n",
        "            common_neighbors = len(src_neighbors.intersection(dst_neighbors))\n",
        "            \n",
        "            # Combine features\n",
        "            pair_features = np.concatenate([\n",
        "                src_features,\n",
        "                dst_features,\n",
        "                [distance, common_neighbors]\n",
        "            ])\n",
        "            \n",
        "            features.append(pair_features)\n",
        "            \n",
        "        return np.array(features)\n",
        "\n",
        "def comprehensive_evaluation(model, data, num_epochs=200, k_folds=5):\n",
        "    \"\"\"Comprehensive evaluation comparing GNN with classical methods\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "    \n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Running comprehensive evaluation with classical baselines...\")\n",
        "    \n",
        "    # Initialize classical methods\n",
        "    classical_routing = ClassicalRoutingMethods(data)\n",
        "    ml_baselines = MachineLearningBaselines(data)\n",
        "    \n",
        "    all_results = []\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "    \n",
        "    # Prepare edge data - handle the undirected edge issue\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    edge_attr = data.edge_attr.cpu().numpy()\n",
        "    \n",
        "    # Create unique edges from the edge_index\n",
        "    unique_edges = set()\n",
        "    edge_to_idx = {}\n",
        "    \n",
        "    # Since edges are undirected, we only take the first half\n",
        "    num_unique_edges = edge_attr.shape[0]\n",
        "    \n",
        "    for i in range(num_unique_edges):\n",
        "        if i < edge_index.shape[1]:\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            edge = tuple(sorted([src, dst]))\n",
        "            if edge not in unique_edges:\n",
        "                unique_edges.add(edge)\n",
        "                edge_to_idx[edge] = i\n",
        "    \n",
        "    unique_edges = list(unique_edges)\n",
        "    print(f\"Found {len(unique_edges)} unique edges\")\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(unique_edges)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "        \n",
        "        train_edges = [unique_edges[i] for i in train_idx]\n",
        "        val_edges = [unique_edges[i] for i in val_idx]\n",
        "        \n",
        "        # Prepare training data\n",
        "        train_edge_list = []\n",
        "        train_attr_list = []\n",
        "        \n",
        "        for edge in train_edges:\n",
        "            train_edge_list.append([edge[0], edge[1]])\n",
        "            attr_idx = edge_to_idx[edge]\n",
        "            train_attr_list.append(edge_attr[attr_idx])\n",
        "        \n",
        "        val_edge_list = []\n",
        "        val_attr_list = []\n",
        "        \n",
        "        for edge in val_edges:\n",
        "            val_edge_list.append([edge[0], edge[1]])\n",
        "            attr_idx = edge_to_idx[edge]\n",
        "            val_attr_list.append(edge_attr[attr_idx])\n",
        "        \n",
        "        if not train_edge_list or not val_edge_list:\n",
        "            print(f\"  Skipping fold {fold + 1} due to empty train or validation set\")\n",
        "            continue\n",
        "        \n",
        "        train_edge_index = torch.tensor(train_edge_list, dtype=torch.long).t().to(device)\n",
        "        train_edge_attr = torch.tensor(train_attr_list, dtype=torch.float).to(device)\n",
        "        val_edge_index = torch.tensor(val_edge_list, dtype=torch.long).t().to(device)\n",
        "        val_edge_attr = torch.tensor(val_attr_list, dtype=torch.float).to(device)\n",
        "        \n",
        "        # Train GNN model\n",
        "        print(\"Training GNN model...\")\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "        \n",
        "        # Simplified training loop for comparison\n",
        "        for epoch in range(min(num_epochs, 50)):  # Reduced for faster comparison\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            z, edge_features = model(data.x, train_edge_index, train_edge_attr)\n",
        "            pos_out = model.decode(z, edge_features, train_edge_index)\n",
        "            \n",
        "            neg_edge_index = negative_sampling(\n",
        "                train_edge_index,\n",
        "                num_nodes=data.num_nodes,\n",
        "                num_neg_samples=train_edge_index.size(1)\n",
        "            )\n",
        "            \n",
        "            neg_out = model.decode(z, edge_features, neg_edge_index)\n",
        "            loss = bce_with_logits_loss(pos_out, neg_out)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"  GNN Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "        \n",
        "        # Evaluate all methods\n",
        "        print(\"Evaluating all methods...\")\n",
        "        \n",
        "        # Prepare test data\n",
        "        val_pairs = [(val_edge_index[0, i].item(), val_edge_index[1, i].item()) \n",
        "                     for i in range(val_edge_index.size(1))]\n",
        "        train_pairs = [(train_edge_index[0, i].item(), train_edge_index[1, i].item()) \n",
        "                       for i in range(train_edge_index.size(1))]\n",
        "        \n",
        "        # Generate negative samples for fair comparison\n",
        "        val_neg_edge_index = negative_sampling(\n",
        "            val_edge_index,\n",
        "            num_nodes=data.num_nodes,\n",
        "            num_neg_samples=val_edge_index.size(1)\n",
        "        )\n",
        "        val_neg_pairs = [(val_neg_edge_index[0, i].item(), val_neg_edge_index[1, i].item()) \n",
        "                         for i in range(val_neg_edge_index.size(1))]\n",
        "        \n",
        "        all_val_pairs = val_pairs + val_neg_pairs\n",
        "        true_labels = [1] * len(val_pairs) + [0] * len(val_neg_pairs)\n",
        "        \n",
        "        train_labels = [1] * len(train_pairs)\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        # 1. GNN Evaluation\n",
        "        print(\"  Evaluating GNN...\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            z, edge_features = model(data.x, val_edge_index, val_edge_attr)\n",
        "            pos_out = model.decode(z, edge_features, val_edge_index)\n",
        "            neg_out = model.decode(z, edge_features, val_neg_edge_index)\n",
        "            \n",
        "            gnn_pred = torch.cat([pos_out, neg_out]).cpu().numpy()\n",
        "            gnn_auc = roc_auc_score(true_labels, gnn_pred)\n",
        "            gnn_ap = average_precision_score(true_labels, gnn_pred)\n",
        "            \n",
        "        results['GNN'] = {\n",
        "            'auc': gnn_auc,\n",
        "            'ap': gnn_ap,\n",
        "            'predictions': gnn_pred\n",
        "        }\n",
        "        \n",
        "        # 2. Classical Routing Methods\n",
        "        print(\"  Evaluating classical routing methods...\")\n",
        "        \n",
        "        # Dijkstra\n",
        "        start_time = time.time()\n",
        "        dijkstra_pred = classical_routing.dijkstra_routing(all_val_pairs)\n",
        "        dijkstra_time = time.time() - start_time\n",
        "        \n",
        "        # Normalize predictions for fair comparison\n",
        "        if np.max(dijkstra_pred) > 0:\n",
        "            dijkstra_pred = dijkstra_pred / np.max(dijkstra_pred)\n",
        "        \n",
        "        dijkstra_auc = roc_auc_score(true_labels, dijkstra_pred) if len(np.unique(dijkstra_pred)) > 1 else 0.5\n",
        "        dijkstra_ap = average_precision_score(true_labels, dijkstra_pred)\n",
        "        \n",
        "        results['Dijkstra'] = {\n",
        "            'auc': dijkstra_auc,\n",
        "            'ap': dijkstra_ap,\n",
        "            'time': dijkstra_time,\n",
        "            'predictions': dijkstra_pred\n",
        "        }\n",
        "        \n",
        "        # MST\n",
        "        start_time = time.time()\n",
        "        mst_pred = classical_routing.minimum_spanning_tree(all_val_pairs)\n",
        "        mst_time = time.time() - start_time\n",
        "        \n",
        "        if np.max(mst_pred) > 0:\n",
        "            mst_pred = mst_pred / np.max(mst_pred)\n",
        "        \n",
        "        mst_auc = roc_auc_score(true_labels, mst_pred) if len(np.unique(mst_pred)) > 1 else 0.5\n",
        "        mst_ap = average_precision_score(true_labels, mst_pred)\n",
        "        \n",
        "        results['MST'] = {\n",
        "            'auc': mst_auc,\n",
        "            'ap': mst_ap,\n",
        "            'time': mst_time,\n",
        "            'predictions': mst_pred\n",
        "        }\n",
        "        \n",
        "        # Greedy Best-First\n",
        "        start_time = time.time()\n",
        "        greedy_pred = classical_routing.greedy_best_first(all_val_pairs)\n",
        "        greedy_time = time.time() - start_time\n",
        "        \n",
        "        if np.max(greedy_pred) > 0:\n",
        "            greedy_pred = greedy_pred / np.max(greedy_pred)\n",
        "        \n",
        "        greedy_auc = roc_auc_score(true_labels, greedy_pred) if len(np.unique(greedy_pred)) > 1 else 0.5\n",
        "        greedy_ap = average_precision_score(true_labels, greedy_pred)\n",
        "        \n",
        "        results['Greedy'] = {\n",
        "            'auc': greedy_auc,\n",
        "            'ap': greedy_ap,\n",
        "            'time': greedy_time,\n",
        "            'predictions': greedy_pred\n",
        "        }\n",
        "        \n",
        "        # 3. Machine Learning Baselines\n",
        "        print(\"  Evaluating ML baselines...\")\n",
        "        \n",
        "        # Random Forest\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            rf_pred = ml_baselines.random_forest_predictor(\n",
        "                train_pairs, all_val_pairs, train_labels, true_labels\n",
        "            )\n",
        "            rf_time = time.time() - start_time\n",
        "            \n",
        "            rf_auc = roc_auc_score(true_labels, rf_pred)\n",
        "            rf_ap = average_precision_score(true_labels, rf_pred)\n",
        "            \n",
        "            results['Random Forest'] = {\n",
        "                'auc': rf_auc,\n",
        "                'ap': rf_ap,\n",
        "                'time': rf_time,\n",
        "                'predictions': rf_pred\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"    Random Forest failed: {e}\")\n",
        "            results['Random Forest'] = {'auc': 0.5, 'ap': 0.0, 'time': 0.0}\n",
        "        \n",
        "        # Linear Regression\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            lr_pred = ml_baselines.linear_regression_predictor(\n",
        "                train_pairs, all_val_pairs, train_labels, true_labels\n",
        "            )\n",
        "            lr_time = time.time() - start_time\n",
        "            \n",
        "            lr_auc = roc_auc_score(true_labels, lr_pred)\n",
        "            lr_ap = average_precision_score(true_labels, lr_pred)\n",
        "            \n",
        "            results['Linear Regression'] = {\n",
        "                'auc': lr_auc,\n",
        "                'ap': lr_ap,\n",
        "                'time': lr_time,\n",
        "                'predictions': lr_pred\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"    Linear Regression failed: {e}\")\n",
        "            results['Linear Regression'] = {'auc': 0.5, 'ap': 0.0, 'time': 0.0}\n",
        "        \n",
        "        # 4. Genetic Algorithm (reduced generations for speed)\n",
        "        print(\"  Evaluating Genetic Algorithm...\")\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            ga_optimizer = GeneticAlgorithmOptimizer(\n",
        "                data, population_size=20, generations=30\n",
        "            )\n",
        "            ga_pred = ga_optimizer.optimize(all_val_pairs)\n",
        "            ga_time = time.time() - start_time\n",
        "            \n",
        "            if len(ga_pred) > 0 and np.max(ga_pred) > 0:\n",
        "                ga_pred = ga_pred / np.max(ga_pred)\n",
        "            \n",
        "            ga_auc = roc_auc_score(true_labels, ga_pred) if len(np.unique(ga_pred)) > 1 else 0.5\n",
        "            ga_ap = average_precision_score(true_labels, ga_pred)\n",
        "            \n",
        "            results['Genetic Algorithm'] = {\n",
        "                'auc': ga_auc,\n",
        "                'ap': ga_ap,\n",
        "                'time': ga_time,\n",
        "                'predictions': ga_pred\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"    Genetic Algorithm failed: {e}\")\n",
        "            results['Genetic Algorithm'] = {'auc': 0.5, 'ap': 0.0, 'time': 0.0}\n",
        "        \n",
        "        # Print fold results\n",
        "        print(f\"\\n  Fold {fold + 1} Results:\")\n",
        "        for method, metrics in results.items():\n",
        "            print(f\"    {method}: AUC = {metrics['auc']:.4f}, AP = {metrics['ap']:.4f}\")\n",
        "        \n",
        "        all_results.append(results)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def visualize_comparative_results(results, network_data, save_path='qkd_comparative_results'):\n",
        "    \"\"\"Create comprehensive comparative visualizations\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    save_path = f\"{save_path}_{timestamp}\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    \n",
        "    # Extract method names\n",
        "    methods = list(results[0].keys())\n",
        "    \n",
        "    # Aggregate results across folds\n",
        "    aggregated_results = {}\n",
        "    for method in methods:\n",
        "        auc_scores = [fold[method]['auc'] for fold in results if method in fold]\n",
        "        ap_scores = [fold[method]['ap'] for fold in results if method in fold]\n",
        "        \n",
        "        aggregated_results[method] = {\n",
        "            'auc_mean': np.mean(auc_scores),\n",
        "            'auc_std': np.std(auc_scores),\n",
        "            'ap_mean': np.mean(ap_scores),\n",
        "            'ap_std': np.std(ap_scores),\n",
        "            'auc_scores': auc_scores,\n",
        "            'ap_scores': ap_scores\n",
        "        }\n",
        "    \n",
        "    # Create comparative plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # AUC Comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    method_names = list(aggregated_results.keys())\n",
        "    auc_means = [aggregated_results[m]['auc_mean'] for m in method_names]\n",
        "    auc_stds = [aggregated_results[m]['auc_std'] for m in method_names]\n",
        "    \n",
        "    bars1 = ax1.bar(method_names, auc_means, yerr=auc_stds, capsize=5, alpha=0.7)\n",
        "    ax1.set_title('AUC Score Comparison')\n",
        "    ax1.set_ylabel('AUC Score')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Highlight best method\n",
        "    best_auc_idx = np.argmax(auc_means)\n",
        "    bars1[best_auc_idx].set_color('red')\n",
        "    bars1[best_auc_idx].set_alpha(1.0)\n",
        "    \n",
        "    # AP Comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    ap_means = [aggregated_results[m]['ap_mean'] for m in method_names]\n",
        "    ap_stds = [aggregated_results[m]['ap_std'] for m in method_names]\n",
        "    \n",
        "    bars2 = ax2.bar(method_names, ap_means, yerr=ap_stds, capsize=5, alpha=0.7)\n",
        "    ax2.set_title('Average Precision Comparison')\n",
        "    ax2.set_ylabel('Average Precision')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Highlight best method\n",
        "    best_ap_idx = np.argmax(ap_means)\n",
        "    bars2[best_ap_idx].set_color('red')\n",
        "    bars2[best_ap_idx].set_alpha(1.0)\n",
        "    \n",
        "    # Box plots for score distributions\n",
        "    ax3 = axes[1, 0]\n",
        "    auc_data = [aggregated_results[m]['auc_scores'] for m in method_names]\n",
        "    box1 = ax3.boxplot(auc_data, labels=method_names, patch_artist=True)\n",
        "    ax3.set_title('AUC Score Distribution')\n",
        "    ax3.set_ylabel('AUC Score')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    ax4 = axes[1, 1]\n",
        "    ap_data = [aggregated_results[m]['ap_scores'] for m in method_names]\n",
        "    box2 = ax4.boxplot(ap_data, labels=method_names, patch_artist=True)\n",
        "    ax4.set_title('Average Precision Distribution')\n",
        "    ax4.set_ylabel('Average Precision')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/method_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # Performance improvement analysis\n",
        "    gnn_auc = aggregated_results['GNN']['auc_mean']\n",
        "    gnn_ap = aggregated_results['GNN']['ap_mean']\n",
        "    \n",
        "    improvements = {}\n",
        "    for method in method_names:\n",
        "        if method != 'GNN':\n",
        "            auc_improvement = ((gnn_auc - aggregated_results[method]['auc_mean']) / \n",
        "                             aggregated_results[method]['auc_mean']) * 100\n",
        "            ap_improvement = ((gnn_ap - aggregated_results[method]['ap_mean']) / \n",
        "                            aggregated_results[method]['ap_mean']) * 100\n",
        "            \n",
        "            improvements[method] = {\n",
        "                'auc_improvement': auc_improvement,\n",
        "                'ap_improvement': ap_improvement\n",
        "            }\n",
        "    \n",
        "    # Statistical significance testing\n",
        "    from scipy import stats\n",
        "    \n",
        "    statistical_tests = {}\n",
        "    gnn_auc_scores = aggregated_results['GNN']['auc_scores']\n",
        "    gnn_ap_scores = aggregated_results['GNN']['ap_scores']\n",
        "    \n",
        "    for method in method_names:\n",
        "        if method != 'GNN':\n",
        "            method_auc_scores = aggregated_results[method]['auc_scores']\n",
        "            method_ap_scores = aggregated_results[method]['ap_scores']\n",
        "            \n",
        "            # Perform t-tests\n",
        "            auc_tstat, auc_pvalue = stats.ttest_rel(gnn_auc_scores, method_auc_scores)\n",
        "            ap_tstat, ap_pvalue = stats.ttest_rel(gnn_ap_scores, method_ap_scores)\n",
        "            \n",
        "            statistical_tests[method] = {\n",
        "                'auc_tstat': float(auc_tstat),\n",
        "                'auc_pvalue': float(auc_pvalue),\n",
        "                'ap_tstat': float(ap_tstat),\n",
        "                'ap_pvalue': float(ap_pvalue),\n",
        "                'auc_significant': bool(auc_pvalue < 0.05),  # Convert to native bool\n",
        "                'ap_significant': bool(ap_pvalue < 0.05)     # Convert to native bool\n",
        "            }\n",
        "    \n",
        "    # Generate comprehensive report\n",
        "    report = {\n",
        "        'summary': {\n",
        "            'best_method_auc': method_names[best_auc_idx],\n",
        "            'best_auc_score': float(auc_means[best_auc_idx]),\n",
        "            'best_method_ap': method_names[best_ap_idx],\n",
        "            'best_ap_score': float(ap_means[best_ap_idx]),\n",
        "            'gnn_rank_auc': int(sorted(auc_means, reverse=True).index(gnn_auc) + 1),\n",
        "            'gnn_rank_ap': int(sorted(ap_means, reverse=True).index(gnn_ap) + 1)\n",
        "        },\n",
        "        'detailed_results': {},\n",
        "        'improvements_over_classical': improvements,\n",
        "        'statistical_significance': statistical_tests,\n",
        "        'network_characteristics': {\n",
        "            'num_nodes': int(network_data.num_nodes),\n",
        "            'num_edges': int(network_data.edge_index.size(1) // 2),\n",
        "            'avg_degree': float(network_data.edge_index.size(1) / network_data.num_nodes),\n",
        "            'avg_key_rate': float(network_data.edge_attr[:, 0].mean()),\n",
        "            'avg_qber': float(network_data.edge_attr[:, 1].mean())\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Add detailed results for each method\n",
        "    for method in method_names:\n",
        "        report['detailed_results'][method] = {\n",
        "            'auc_mean': float(aggregated_results[method]['auc_mean']),\n",
        "            'auc_std': float(aggregated_results[method]['auc_std']),\n",
        "            'ap_mean': float(aggregated_results[method]['ap_mean']),\n",
        "            'ap_std': float(aggregated_results[method]['ap_std'])\n",
        "        }\n",
        "    \n",
        "    # Save report\n",
        "    with open(f\"{save_path}/comparative_analysis_report.json\", 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"COMPARATIVE ANALYSIS SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Best AUC: {report['summary']['best_method_auc']} ({report['summary']['best_auc_score']:.4f})\")\n",
        "    print(f\"Best AP:  {report['summary']['best_method_ap']} ({report['summary']['best_ap_score']:.4f})\")\n",
        "    print(f\"GNN Rank (AUC): {report['summary']['gnn_rank_auc']}/{len(method_names)}\")\n",
        "    print(f\"GNN Rank (AP):  {report['summary']['gnn_rank_ap']}/{len(method_names)}\")\n",
        "    \n",
        "    print(f\"\\nGNN Improvements over Classical Methods:\")\n",
        "    for method, improvement in improvements.items():\n",
        "        print(f\"  vs {method}:\")\n",
        "        print(f\"    AUC: {improvement['auc_improvement']:+.2f}%\")\n",
        "        print(f\"    AP:  {improvement['ap_improvement']:+.2f}%\")\n",
        "    \n",
        "    print(f\"\\nStatistical Significance (p < 0.05):\")\n",
        "    for method, test in statistical_tests.items():\n",
        "        print(f\"  vs {method}:\")\n",
        "        print(f\"    AUC: {'Significant' if test['auc_significant'] else 'Not significant'} (p={test['auc_pvalue']:.4f})\")\n",
        "        print(f\"    AP:  {'Significant' if test['ap_significant'] else 'Not significant'} (p={test['ap_pvalue']:.4f})\")\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Update the main execution to use comprehensive evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate network\n",
        "    print(\"Generating QKD network...\")\n",
        "    network = AdvancedQKDNetwork(num_nodes=30)  # Reduced for faster comparison\n",
        "    data = network.generate_graph_data()\n",
        "    \n",
        "    # Create model\n",
        "    print(\"Creating GNN model...\")\n",
        "    model = AdvancedQKDLinkPredictor(\n",
        "        in_channels=data.x.size(1),\n",
        "        edge_attr_channels=data.edge_attr.size(1)\n",
        "    )\n",
        "    \n",
        "    # Run comprehensive evaluation\n",
        "    print(\"Running comprehensive evaluation...\")\n",
        "    comparative_results = comprehensive_evaluation(model, data, num_epochs=50, k_folds=3)\n",
        "    \n",
        "    # Generate comparative analysis\n",
        "    print(\"Generating comparative analysis...\")\n",
        "    analysis_report = visualize_comparative_results(comparative_results, data)\n",
        "    \n",
        "    print(f\"\\nComparative analysis complete!\")\n",
        "    print(f\"Results demonstrate the relative performance of GNN vs classical methods.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
